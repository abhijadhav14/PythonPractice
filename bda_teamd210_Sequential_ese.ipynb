{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhijadhav14/PythonPractice/blob/main/bda_teamd210_Sequential_ese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.423684Z",
          "iopub.execute_input": "2022-10-04T12:29:06.424156Z",
          "iopub.status.idle": "2022-10-04T12:29:06.446089Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.424120Z",
          "shell.execute_reply": "2022-10-04T12:29:06.444977Z"
        },
        "trusted": true,
        "id": "A5fqOmq11io2",
        "outputId": "53e14544-d966-4693-9713-95cafa3f88fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results__.html\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__resultx__.html\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__notebook__.ipynb\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__output__.json\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/punt_play_data.csv\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/custom.css\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___20_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___18_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___27_2.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___19_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___95_0.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___73_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___25_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___32_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___21_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___87_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___16_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___24_1.png\n/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/__results___files/__results___17_1.png\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk1-6.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/player_punt_data.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2017-post.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2016-post.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/play_information.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/play_player_role_data.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/First and Future Data Manual 12062018.docx\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk13-17.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/Data Manual 11302018.docx\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk1-6.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/First and Future Data Manual 12032018.docx\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2016-reg-wk7-12.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/video_review.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2017-pre.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/video_footage-injury.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/video_footage-control.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2016-pre.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/game_data.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk7-12.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS-2017-reg-wk13-17.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2017-reg-wk13-17.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2017-post.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2016-pre.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2016-reg-wk7-12.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2017-reg-wk1-6.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2017-pre.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2017-reg-wk7-12.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2016-post.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2016-reg-wk13-17.csv\n/kaggle/input/NFL-Punt-Analytics-Competition/NGS/2016-reg-wk1-6.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reorient(df, flip_left=False):\n",
        "\n",
        "\n",
        "    df = df.drop_duplicates(subset=['GameKey',\"PlayID\",'GSISID'])\n",
        "\n",
        "    return_team_role = ['PDL1','PDL2','PDL3','PDL4','PDL5','PDL6','PDM','PDR1','PDR2','PDR3','PDR4','PDR5', 'PDR6','PFB','PLL','PLL1','PLL2','PLL3','PLM','PLR','PLR1','PLR2','PLR3','VL','VLi','VLo','VR','VRi','VRo','PR']\n",
        "\n",
        "    df['IsReturner'] = df.Role == 'PR'\n",
        "    df['IsReturnTeam'] = df.Role.isin(return_team_role)\n",
        "\n",
        "    punt_team_data = df[df.IsReturnTeam == False].groupby(['GameKey','PlayID'])['x'].mean().reset_index()\n",
        "    punt_team_data.columns = ['GameKey','PlayID','punt_team_x']\n",
        "    returner_team_data = df[df.Role == 'PR'].groupby(['GameKey','PlayID'])['x'].mean().reset_index()\n",
        "    returner_team_data.columns = ['GameKey','PlayID','returner_x']\n",
        "    play_data = pd.merge(punt_team_data,returner_team_data,how='inner')\n",
        "    play_data['ToLeft'] = play_data.returner_x < play_data.punt_team_x\n",
        "    df = pd.merge(df,play_data[['GameKey','PlayID','ToLeft']],on=['GameKey','PlayID'])\n",
        "\n",
        "    df.loc[df.ToLeft, 'x'] = 120 - df.loc[df.ToLeft, 'x']\n",
        "    df.loc[df.ToLeft, 'y'] = 160 / 3 - df.loc[df.ToLeft, 'y']\n",
        "    df.loc[df.ToLeft, 'o'] = np.mod(180 + df.loc[df.ToLeft, 'o'], 360)\n",
        "    df['dir'] = 90 - df.dir\n",
        "    df.loc[df.ToLeft, 'dir'] = np.mod(180 + df.loc[df.ToLeft, 'dir'], 360)\n",
        "    df.loc[df.IsReturnTeam, 'dir'] = df.loc[df.IsReturnTeam, 'dir'].fillna(0).values\n",
        "    df.loc[~df.IsReturnTeam, 'dir'] = df.loc[~df.IsReturnTeam, 'dir'].fillna(180).values\n",
        "\n",
        "    if flip_left:\n",
        "        tmp = df[df['IsReturner']].copy()\n",
        "        # df['left'] = df.Y < 160/6\n",
        "        tmp['left'] = tmp.dir < 0\n",
        "        df = df.merge(tmp[['GameKey','PlayID', 'left']], how='left', on=['GameKey','PlayID'])\n",
        "        df['y'] = df.y\n",
        "        df.loc[df[\"left\"], 'y'] = 160 / 3 - df.loc[df[\"left\"], 'y']\n",
        "        df['dir'] = df.dir\n",
        "        df.loc[df[\"left\"], 'dir'] = np.mod(- df.loc[df[\"left\"], 'dir'], 360)\n",
        "        df.drop('left', axis=1, inplace=True)\n",
        "\n",
        "    df[\"S\"] = df[\"dis\"] * 10\n",
        "    df['x_dir'] = np.cos((np.pi / 180) * df.dir)\n",
        "    df['y_dir'] = np.sin((np.pi / 180) * df.dir)\n",
        "    df['x_S'] = df.x_dir * df.S\n",
        "    df['y_S'] = df.y_dir * df.S\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def merge_returnerfeats(df):\n",
        "    returner_feats = df[df['Role'] == 'PR'].drop_duplicates()\n",
        "    returner_feats = returner_feats[['GameKey',\"PlayID\", \"x\", \"y\", \"x_S\", \"y_S\"]]\n",
        "    returner_feats = returner_feats.rename(\n",
        "        columns={\"x\": \"Returner_x\", \"y\": \"Returner_y\", \"x_S\": \"Returner_x_S\", \"y_S\": \"Returner_y_S\"})\n",
        "    df = df.merge(returner_feats, how=\"left\", on=['GameKey',\"PlayID\"]).drop_duplicates(subset=['GameKey',\"PlayID\",'GSISID'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def scaling(feats, sctype=\"standard\"):\n",
        "    v1 = []\n",
        "    v2 = []\n",
        "    for i in range(feats.shape[1]):\n",
        "        feats_ = feats[:, i, :]\n",
        "        if sctype == \"standard\":\n",
        "            mean_ = np.mean(feats_)\n",
        "            std_ = np.std(feats_)\n",
        "            feats[:, i, :] -= mean_\n",
        "            feats[:, i, :] /= std_\n",
        "            v1.append(mean_)\n",
        "            v2.append(std_)\n",
        "        elif sctype == \"minmax\":\n",
        "            max_ = np.max(feats_)\n",
        "            min_ = np.min(feats_)\n",
        "            feats[:, i, :] = (feats_ - min_) / (max_ - min_)\n",
        "            v1.append(max_)\n",
        "            v2.append(min_)\n",
        "\n",
        "    return feats, v1, v2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.448435Z",
          "iopub.execute_input": "2022-10-04T12:29:06.449228Z",
          "iopub.status.idle": "2022-10-04T12:29:06.481695Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.449186Z",
          "shell.execute_reply": "2022-10-04T12:29:06.480231Z"
        },
        "trusted": true,
        "id": "7o1bkiK31io3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features(df):\n",
        "    xysdir_o = df[(df.IsReturnTeam == True) & (df.IsReturner == False)][['x','y','x_S','y_S']].values\n",
        "    xysdir_rush = df[df.IsReturner == True][['x','y','x_S','y_S']].values\n",
        "    xysdir_d = df[df.IsReturnTeam == False][['x','y','x_S','y_S']].values\n",
        "\n",
        "    off_x = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['x'].apply(np.array))\n",
        "    def_x = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['x'].apply(np.array))\n",
        "    off_y = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['y'].apply(np.array))\n",
        "    def_y = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['y'].apply(np.array))\n",
        "    off_sx = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['x_S'].apply(np.array))\n",
        "    def_sx = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['x_S'].apply(np.array))\n",
        "    off_sy = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['y_S'].apply(np.array))\n",
        "    def_sy = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['y_S'].apply(np.array))\n",
        "\n",
        "    player_vector = []\n",
        "    player_vector_aug = []\n",
        "    for play in range(len(off_x)):\n",
        "        player_feat, player_feat_aug = player_feature(off_x[play],def_x[play],off_y[play],def_y[play],off_sx[play],def_sx[play],\n",
        "                                     off_sy[play],def_sy[play],xysdir_rush[play])\n",
        "        player_vector.append(player_feat)\n",
        "        player_vector_aug.append(player_feat_aug)\n",
        "\n",
        "    return np.array(player_vector), np.array(player_vector_aug)\n",
        "\n",
        "\n",
        "def player_feature(off_x,def_x,off_y,def_y,off_sx,def_sx,off_sy,def_sy,xysdir_rush):\n",
        "    if(len(off_x)<10):\n",
        "        off_x = np.pad(off_x,(10-len(off_x),0), 'mean' )\n",
        "        off_y = np.pad(off_y,(10-len(off_y),0), 'mean' )\n",
        "        off_sx = np.pad(off_sx,(10-len(off_sx),0), 'mean' )\n",
        "        off_sy = np.pad(off_sy,(10-len(off_sy),0), 'mean' )\n",
        "    if(len(def_x)<11):\n",
        "        def_x = np.pad(def_x,(11-len(def_x),0), 'mean' )\n",
        "        def_y = np.pad(def_y,(11-len(def_y),0), 'mean' )\n",
        "        def_sx = np.pad(def_sx,(11-len(def_sx),0), 'mean' )\n",
        "        def_sy = np.pad(def_sy,(11-len(def_sy),0), 'mean' )\n",
        "\n",
        "    dist_def_off_x = def_x.reshape(-1,1)-off_x.reshape(1,-1)\n",
        "    dist_def_off_sx = def_sx.reshape(-1,1)-off_sx.reshape(1,-1)\n",
        "    dist_def_off_y = def_y.reshape(-1,1)-off_y.reshape(1,-1)\n",
        "    dist_def_off_sy = def_sy.reshape(-1,1)-off_sy.reshape(1,-1)\n",
        "    dist_def_rush_x = def_x.reshape(-1,1)-np.repeat(xysdir_rush[0],10).reshape(1,-1)\n",
        "    dist_def_rush_y = def_y.reshape(-1,1)-np.repeat(xysdir_rush[1],10).reshape(1,-1)\n",
        "    dist_def_rush_sx = def_sx.reshape(-1,1)-np.repeat(xysdir_rush[2],10).reshape(1,-1)\n",
        "    dist_def_rush_sy = def_sy.reshape(-1,1)-np.repeat(xysdir_rush[3],10).reshape(1,-1)\n",
        "    def_sx = np.repeat(def_sx,10).reshape(11,-1)\n",
        "    def_sy = np.repeat(def_sy,10).reshape(11,-1)\n",
        "    feats = [dist_def_off_x, dist_def_off_sx, dist_def_off_y, dist_def_off_sy, dist_def_rush_x, dist_def_rush_y,\n",
        "            dist_def_rush_sx, dist_def_rush_sy, def_sx, def_sy]\n",
        "    feats_aug = [dist_def_off_x, dist_def_off_sx, -1*dist_def_off_y, -1*dist_def_off_sy, dist_def_rush_x,\n",
        "                 -1*dist_def_rush_y, dist_def_rush_sx, -1*dist_def_rush_sy, def_sx, -1*def_sy]\n",
        "\n",
        "    return np.stack(feats), np.stack(feats_aug)\n",
        "\n",
        "# def create_features_nearest(df):\n",
        "#     xysdir_o = df[(df.IsReturnTeam == True) & (df.IsReturner == False)][['x','y','x_S','y_S']].values\n",
        "#     xysdir_rush = df[df.IsReturner == True][['x','y','x_S','y_S']].values\n",
        "#     xysdir_d = df[df.IsReturnTeam == False][['x','y','x_S','y_S']].values\n",
        "\n",
        "#     off_x = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['x'].apply(np.array))\n",
        "#     def_x = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['x'].apply(np.array))\n",
        "#     off_y = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['y'].apply(np.array))\n",
        "#     def_y = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['y'].apply(np.array))\n",
        "#     off_sx = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['x_S'].apply(np.array))\n",
        "#     def_sx = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['x_S'].apply(np.array))\n",
        "#     off_sy = np.array(df[(df.IsReturnTeam == True) & (df.IsReturner == False)].groupby(['GameKey','PlayID'])['y_S'].apply(np.array))\n",
        "#     def_sy = np.array(df[(df.IsReturnTeam == False) ].groupby(['GameKey','PlayID'])['y_S'].apply(np.array))\n",
        "\n",
        "#     player_vector = []\n",
        "#     player_vector_aug = []\n",
        "#     for play in range(len(off_x)):\n",
        "#         player_feat, player_feat_aug = player_feature_nearest(off_x[play],def_x[play],off_y[play],def_y[play],off_sx[play],def_sx[play],\n",
        "#                                      off_sy[play],def_sy[play],xysdir_rush[play])\n",
        "#         player_vector.append(player_feat)\n",
        "#         player_vector_aug.append(player_feat_aug)\n",
        "\n",
        "#     return np.array(player_vector), np.array(player_vector_aug)\n",
        "\n",
        "\n",
        "# def player_feature_nearest(off_x,def_x,off_y,def_y,off_sx,def_sx,off_sy,def_sy,xysdir_rush):\n",
        "#     if(len(off_x)<10):\n",
        "#         off_x = np.pad(off_x,(10-len(off_x),0), 'mean' )\n",
        "#         off_y = np.pad(off_y,(10-len(off_y),0), 'mean' )\n",
        "#         off_sx = np.pad(off_sx,(10-len(off_sx),0), 'mean' )\n",
        "#         off_sy = np.pad(off_sy,(10-len(off_sy),0), 'mean' )\n",
        "#     if(len(def_x)<11):\n",
        "#         def_x = np.pad(def_x,(11-len(def_x),0), 'mean' )\n",
        "#         def_y = np.pad(def_y,(11-len(def_y),0), 'mean' )\n",
        "#         def_sx = np.pad(def_sx,(11-len(def_sx),0), 'mean' )\n",
        "#         def_sy = np.pad(def_sy,(11-len(def_sy),0), 'mean' )\n",
        "\n",
        "#     dist_def_rush_x = (def_x.reshape(-1,1)-xysdir_rush[0]).reshape(1,-1)\n",
        "#     dist_def_rush_y = (def_y.reshape(-1,1)-xysdir_rush[1]).reshape(1,-1)\n",
        "#     dist_def_rush_sx = (def_sx.reshape(-1,1)-xysdir_rush[2]).reshape(1,-1)\n",
        "#     dist_def_rush_sy = (def_sy.reshape(-1,1)-xysdir_rush[3]).reshape(1,-1)\n",
        "\n",
        "#     feats = [ dist_def_rush_x, dist_def_rush_y, dist_def_rush_sx, dist_def_rush_sy]\n",
        "#     feats_aug = [ dist_def_rush_x, -1*dist_def_rush_y, dist_def_rush_sx, -1*dist_def_rush_sy]\n",
        "\n",
        "\n",
        "#     return np.stack(feats), np.stack(feats_aug)\n",
        "\n",
        "\n",
        "# def get_def_speed(df):\n",
        "#     df_cp = df[~df.IsOnOffense].copy()\n",
        "#     speed = 10*df_cp[\"dis\"].T.values\n",
        "#     speed = speed.reshape(-1, 1, 1, 11)\n",
        "#     speed = np.repeat(speed, 10, axis=2)\n",
        "\n",
        "#     return speed\n",
        "\n",
        "\n",
        "# def get_dist(df, col1, col2, type=\"defence\"):\n",
        "#     if type == \"defence\":\n",
        "#         df_cp = df[~df.IsOnOffense].copy()\n",
        "#     elif type == \"offence\":\n",
        "#         df_cp = df[df.IsOnOffense].copy()\n",
        "#     dist = np.linalg.norm(df_cp[col1].values - df_cp[col2].values, axis=1)\n",
        "#     dist = dist.T\n",
        "#     dist = dist.reshape(-1, 1, 1, 11)\n",
        "#     dist = np.repeat(dist, 10, axis=2)\n",
        "\n",
        "#     return dist\n",
        "\n",
        "\n",
        "\n",
        "# def dist_def_off(df, n_train, cols):\n",
        "#     off_x = np.array(df[(df.IsOnOffense) & (~train.IsRusher)].groupby('PlayId')['x'].apply(np.array))\n",
        "#     def_x = np.array(df[(~df.IsOnOffense) ].groupby('PlayId')['x'].apply(np.array))\n",
        "#     off_y = np.array(df[(df.IsOnOffense) & (~train.IsRusher)].groupby('PlayId')['y'].apply(np.array))\n",
        "#     def_y = np.array(df[(~df.IsOnOffense) ].groupby('PlayId')['y'].apply(np.array))\n",
        "#     off_xs = np.array(df[(df.IsOnOffense) & (~train.IsRusher)].groupby('PlayId')['x_S'].apply(np.array))\n",
        "#     def_xs = np.array(df[(~df.IsOnOffense) ].groupby('PlayId')['x_S'].apply(np.array))\n",
        "#     off_ys = np.array(df[(df.IsOnOffense) & (~train.IsRusher)].groupby('PlayId')['y_S'].apply(np.array))\n",
        "#     def_ys = np.array(df[(~df.IsOnOffense) ].groupby('PlayId')['y_S'].apply(np.array))\n",
        "#     feats = []\n",
        "#     for play in range(len(off_x)):\n",
        "#         dist_x = off_x[play].reshape(-1, 1) - def_x[play].reshape(1, -1)\n",
        "#         dist_y = off_y[play].reshape(-1, 1) - def_y[play].reshape(1, -1)\n",
        "#         dist = np.concatenate([dist_x[:, :, np.newaxis], dist_y[:, :, np.newaxis]], axis=2)\n",
        "#         dist_xy = np.linalg.norm(dist.astype(np.float64), axis=2)\n",
        "#         dist_xs = off_xs[play].reshape(-1, 1) - def_xs[play].reshape(1, -1)\n",
        "#         dist_ys = off_ys[play].reshape(-1, 1) - def_ys[play].reshape(1, -1)\n",
        "#         dist = np.concatenate([dist_xs[:, :, np.newaxis], dist_ys[:, :, np.newaxis]], axis=2)\n",
        "#         dist_xys = np.linalg.norm(dist.astype(np.float64), axis=2)\n",
        "#         feats.append(np.concatenate([dist_xy[np.newaxis, :], dist_xys[np.newaxis, :]], axis=0))\n",
        "#     return np.array(feats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.611747Z",
          "iopub.execute_input": "2022-10-04T12:29:06.612199Z",
          "iopub.status.idle": "2022-10-04T12:29:06.641640Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.612161Z",
          "shell.execute_reply": "2022-10-04T12:29:06.640692Z"
        },
        "trusted": true,
        "id": "JqEl84Ca1io4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def crps(y_loss_val, y_pred):\n",
        "    #y_true = np.clip(np.cumsum(y_loss_val, axis=1), 0, 1)\n",
        "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
        "    y_pred[:, :99-30] = 0.0\n",
        "    y_pred[:, 50+99:] = 1.0\n",
        "    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_loss_val.shape[0])\n",
        "    crps = np.round(val_s, 6)\n",
        "\n",
        "    return crps\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.643692Z",
          "iopub.execute_input": "2022-10-04T12:29:06.644095Z",
          "iopub.status.idle": "2022-10-04T12:29:06.658533Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.644060Z",
          "shell.execute_reply": "2022-10-04T12:29:06.657010Z"
        },
        "trusted": true,
        "id": "h79oWHzg1io5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def crps(y_loss_val, y_pred):\n",
        "    #y_true = np.clip(np.cumsum(y_loss_val, axis=1), 0, 1)\n",
        "    y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n",
        "    y_pred[:, :99-30] = 0.0\n",
        "    y_pred[:, 50+99:] = 1.0\n",
        "    val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * y_loss_val.shape[0])\n",
        "    crps = np.round(val_s, 6)\n",
        "\n",
        "    return crps"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.659931Z",
          "iopub.execute_input": "2022-10-04T12:29:06.661034Z",
          "iopub.status.idle": "2022-10-04T12:29:06.671516Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.660984Z",
          "shell.execute_reply": "2022-10-04T12:29:06.669941Z"
        },
        "trusted": true,
        "id": "AOY8uv6h1io6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class CnnModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(10, 128, kernel_size=1, stride=1, bias=False),\n",
        "            nn.CELU(inplace=True),\n",
        "            nn.Conv2d(128, 160, kernel_size=1, stride=1, bias=False),\n",
        "            nn.CELU(inplace=True),\n",
        "            nn.Conv2d(160, 128, kernel_size=1, stride=1, bias=False),\n",
        "            nn.CELU(inplace=True)\n",
        "        )\n",
        "        self.pool1 = nn.AdaptiveAvgPool2d((1, 11))\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 160, kernel_size=(1, 1), stride=1, bias=False),\n",
        "            nn.CELU(inplace=True),\n",
        "            nn.BatchNorm2d(160),\n",
        "            nn.Conv2d(160, 96, kernel_size=(1, 1), stride=1, bias=False),\n",
        "            nn.CELU(inplace=True),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.Conv2d(96, 96, kernel_size=(1, 1), stride=1, bias=False),\n",
        "            nn.CELU(inplace=True),\n",
        "            nn.BatchNorm2d(96),\n",
        "        )\n",
        "        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.last_linear = nn.Sequential(\n",
        "            Flatten(),\n",
        "            nn.Linear(96, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.last_linear(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:06.676297Z",
          "iopub.execute_input": "2022-10-04T12:29:06.677001Z",
          "iopub.status.idle": "2022-10-04T12:29:08.489408Z",
          "shell.execute_reply.started": "2022-10-04T12:29:06.676957Z",
          "shell.execute_reply": "2022-10-04T12:29:08.487899Z"
        },
        "trusted": true,
        "id": "1eRXqRKq1io6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def seed_torch(seed=1029):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.494670Z",
          "iopub.execute_input": "2022-10-04T12:29:08.495613Z",
          "iopub.status.idle": "2022-10-04T12:29:08.502811Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.495560Z",
          "shell.execute_reply": "2022-10-04T12:29:08.501896Z"
        },
        "trusted": true,
        "id": "atcby0JE1io7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "LOGGER = logging.getLogger()\n",
        "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "\n",
        "\n",
        "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
        "    LOGGER.handlers = []\n",
        "    LOGGER.setLevel(min(stderr_level, file_level))\n",
        "\n",
        "    if stderr:\n",
        "        handler = logging.StreamHandler(sys.stderr)\n",
        "        handler.setFormatter(FORMATTER)\n",
        "        handler.setLevel(stderr_level)\n",
        "        LOGGER.addHandler(handler)\n",
        "\n",
        "    if out_file is not None:\n",
        "        handler = logging.FileHandler(out_file)\n",
        "        handler.setFormatter(FORMATTER)\n",
        "        handler.setLevel(file_level)\n",
        "        LOGGER.addHandler(handler)\n",
        "\n",
        "    LOGGER.info(\"logger set up\")\n",
        "    return LOGGER"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.504361Z",
          "iopub.execute_input": "2022-10-04T12:29:08.505073Z",
          "iopub.status.idle": "2022-10-04T12:29:08.518216Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.505027Z",
          "shell.execute_reply": "2022-10-04T12:29:08.517146Z"
        },
        "trusted": true,
        "id": "vqCInDdE1io7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device,\n",
        "                    steps_upd_logging=500, accumulation_steps=1, scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for step, (x, targets) in enumerate(train_loader):\n",
        "        #x= x.to(device)\n",
        "        #targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x)\n",
        "        #_, targets = targets.max(dim=1)\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:  # Wait for several backward steps\n",
        "            optimizer.step()  # Now we can do an optimizer step\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        if (step + 1) % steps_upd_logging == 0:\n",
        "            LOGGER.info('Train loss on step {} was {}'.format(step + 1, round(total_loss / (step + 1), 5)))\n",
        "\n",
        "\n",
        "    return total_loss / (step + 1)\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0.0\n",
        "    true_ans_list = []\n",
        "    preds_cat = []\n",
        "    for step, (x, targets) in enumerate(val_loader):\n",
        "        #x= x.to(device)\n",
        "        #targets = targets.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        #_, targets = targets.max(dim=1)\n",
        "        loss = criterion(logits, targets)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        targets = targets.float().detach().numpy()\n",
        "        logits = torch.softmax(logits, 1).float().detach().numpy()\n",
        "        true_ans_list.append(targets)\n",
        "        preds_cat.append(logits)\n",
        "\n",
        "        del x, targets, logits\n",
        "        gc.collect()\n",
        "\n",
        "    all_true_ans = np.concatenate(true_ans_list, axis=0)\n",
        "    all_preds = np.concatenate(preds_cat, axis=0)\n",
        "\n",
        "    return all_preds, all_true_ans, val_loss / (step + 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.519895Z",
          "iopub.execute_input": "2022-10-04T12:29:08.520668Z",
          "iopub.status.idle": "2022-10-04T12:29:08.535419Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.520606Z",
          "shell.execute_reply": "2022-10-04T12:29:08.534185Z"
        },
        "trusted": true,
        "id": "skrwFM611io7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CRPSLoss(nn.Module):\n",
        "    def __init__(self, n_class=199):\n",
        "        super().__init__()\n",
        "        self.n_class = n_class\n",
        "        self.mse = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        y_pred = torch.softmax(y_pred, 1)\n",
        "        y_pred = torch.clamp(torch.cumsum(y_pred, 1), 0, 1)\n",
        "        #crps = torch.sum(torch.sum((y_true - y_pred) ** 2, 1), 0) / (self.n_class * y_true.shape[0])\n",
        "        crps = self.mse(y_pred, y_true)\n",
        "        return crps"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.536855Z",
          "iopub.execute_input": "2022-10-04T12:29:08.537550Z",
          "iopub.status.idle": "2022-10-04T12:29:08.552462Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.537514Z",
          "shell.execute_reply": "2022-10-04T12:29:08.551124Z"
        },
        "trusted": true,
        "id": "9cgyXIeK1io8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "\n",
        "class OneCycleLR:\n",
        "    \"\"\" Sets the learing rate of each parameter group by the one cycle learning rate policy\n",
        "    proposed in https://arxiv.org/pdf/1708.07120.pdf.\n",
        "    It is recommended that you set the max_lr to be the learning rate that achieves\n",
        "    the lowest loss in the learning rate range test, and set min_lr to be 1/10 th of max_lr.\n",
        "    So, the learning rate changes like min_lr -> max_lr -> min_lr -> final_lr,\n",
        "    where final_lr = min_lr * reduce_factor.\n",
        "    Note: Currently only supports one parameter group.\n",
        "    Args:\n",
        "        optimizer:             (Optimizer) against which we apply this scheduler\n",
        "        num_steps:             (int) of total number of steps/iterations\n",
        "        lr_range:              (tuple) of min and max values of learning rate\n",
        "        momentum_range:        (tuple) of min and max values of momentum\n",
        "        annihilation_frac:     (float), fracion of steps to annihilate the learning rate\n",
        "        reduce_factor:         (float), denotes the factor by which we annihilate the learning rate at the end\n",
        "        last_step:             (int), denotes the last step. Set to -1 to start training from the beginning\n",
        "    Example:\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "        >>> scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.1, 1.))\n",
        "        >>> for epoch in range(epochs):\n",
        "        >>>     for step in train_dataloader:\n",
        "        >>>         train(...)\n",
        "        >>>         scheduler.step()\n",
        "    Useful resources:\n",
        "        https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
        "        https://medium.com/vitalify-asia/whats-up-with-deep-learning-optimizers-since-adam-5c1d862b9db0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: Optimizer,\n",
        "                 num_steps: int,\n",
        "                 lr_range: tuple = (0.1, 1.),\n",
        "                 momentum_range: tuple = (0.85, 0.95),\n",
        "                 annihilation_frac: float = 0.1,\n",
        "                 reduce_factor: float = 0.01,\n",
        "                 last_step: int = -1):\n",
        "        # Sanity check\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self.min_lr, self.max_lr = lr_range[0], lr_range[1]\n",
        "        assert self.min_lr < self.max_lr, \\\n",
        "            \"Argument lr_range must be (min_lr, max_lr), where min_lr < max_lr\"\n",
        "\n",
        "        self.min_momentum, self.max_momentum = momentum_range[0], momentum_range[1]\n",
        "        assert self.min_momentum < self.max_momentum, \\\n",
        "            \"Argument momentum_range must be (min_momentum, max_momentum), where min_momentum < max_momentum\"\n",
        "\n",
        "        self.num_cycle_steps = int(num_steps * (1. - annihilation_frac))  # Total number of steps in the cycle\n",
        "        self.final_lr = self.min_lr * reduce_factor\n",
        "\n",
        "        self.last_step = last_step\n",
        "\n",
        "        if self.last_step == -1:\n",
        "            self.step()\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns the state of the scheduler as a :class:`dict`.\n",
        "        It contains an entry for every variable in self.__dict__ which\n",
        "        is not the optimizer. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
        "        \"\"\"\n",
        "        return {key: value for key, value in self.__dict__.items() if key != 'optimizer'}\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"\"\"Loads the schedulers state. (Borrowed from _LRScheduler class in torch.optim.lr_scheduler.py)\n",
        "        Arguments:\n",
        "            state_dict (dict): scheduler state. Should be an object returned\n",
        "                from a call to :meth:`state_dict`.\n",
        "        \"\"\"\n",
        "        self.__dict__.update(state_dict)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "    def get_momentum(self):\n",
        "        return self.optimizer.param_groups[0]['momentum']\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Conducts one step of learning rate and momentum update\n",
        "        \"\"\"\n",
        "        current_step = self.last_step + 1\n",
        "        self.last_step = current_step\n",
        "\n",
        "        if current_step <= self.num_cycle_steps // 2:\n",
        "            # Scale up phase\n",
        "            scale = current_step / (self.num_cycle_steps // 2)\n",
        "            lr = self.min_lr + (self.max_lr - self.min_lr) * scale\n",
        "            momentum = self.max_momentum - (self.max_momentum - self.min_momentum) * scale\n",
        "        elif current_step <= self.num_cycle_steps:\n",
        "            # Scale down phase\n",
        "            scale = (current_step - self.num_cycle_steps // 2) / (self.num_cycle_steps - self.num_cycle_steps // 2)\n",
        "            lr = self.max_lr - (self.max_lr - self.min_lr) * scale\n",
        "            momentum = self.min_momentum + (self.max_momentum - self.min_momentum) * scale\n",
        "        elif current_step <= self.num_steps:\n",
        "            # Annihilation phase: only change lr\n",
        "            scale = (current_step - self.num_cycle_steps) / (self.num_steps - self.num_cycle_steps)\n",
        "            lr = self.min_lr - (self.min_lr - self.final_lr) * scale\n",
        "            momentum = None\n",
        "        else:\n",
        "            # Exceeded given num_steps: do nothing\n",
        "            return\n",
        "\n",
        "        self.optimizer.param_groups[0]['lr'] = lr\n",
        "        if momentum:\n",
        "            self.optimizer.param_groups[0]['momentum'] = momentum"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.554458Z",
          "iopub.execute_input": "2022-10-04T12:29:08.554915Z",
          "iopub.status.idle": "2022-10-04T12:29:08.582625Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.554869Z",
          "shell.execute_reply": "2022-10-04T12:29:08.581408Z"
        },
        "trusted": true,
        "id": "M-eWab4Z1io8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from contextlib import contextmanager\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from glob import glob\n",
        "\n",
        "#from trainer import train_one_epoch, validate\n",
        "\n",
        "# ===============\n",
        "# Constants\n",
        "# ===============\n",
        "NGS_path = glob('/kaggle/input/NFL-Punt-Analytics-Competition/NGS/*')\n",
        "DATA_DIR = \"../input/nfl-big-data-bowl-2020\"\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
        "LOGGER_PATH = \"log.txt\"\n",
        "TARGET_COLUMNS = 'return_yards'\n",
        "N_CLASSES = 199\n",
        "\n",
        "# ===============\n",
        "# Settings\n",
        "# ===============\n",
        "SEED = np.random.randint(100000)\n",
        "device = \"cuda\"\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 64\n",
        "TTA = True\n",
        "EXP_ID = \"exp1\"\n",
        "epochs = 50\n",
        "EXP_ID = \"exp1_reproduce\"\n",
        "\n",
        "setup_logger(out_file=LOGGER_PATH)\n",
        "seed_torch(SEED)\n",
        "LOGGER.info(\"seed={}\".format(SEED))\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    LOGGER.info('[{}] done in {} s'.format(name, round(time.time() - t0, 2)))\n",
        "\n",
        "\n",
        "with timer('load data'):\n",
        "    play_information_data = pd.read_csv('/kaggle/input/nfl-punt-analytic-with-suggested-rule-change/punt_play_data.csv')\n",
        "    player_role_data = pd.read_csv('/kaggle/input/NFL-Punt-Analytics-Competition/play_player_role_data.csv')\n",
        "    train = pd.DataFrame({})\n",
        "    for path in NGS_path:\n",
        "        data = pd.read_csv(path)\n",
        "        data = pd.merge(data[data.Event == 'punt_received'],play_information_data[['GameKey' ,'PlayID' ,'return_yards']])\n",
        "        data = pd.merge(data,player_role_data)\n",
        "        train = pd.concat([train,data[data.Event == 'punt_received']])\n",
        "        del data\n",
        "    train = train.dropna(subset=['return_yards'])\n",
        "\n",
        "\n",
        "\n",
        "    #season = train[\"Season_Year\"][::22].values\n",
        "\n",
        "\n",
        "with timer('create features'):\n",
        "    train = reorient(train, flip_left=True)\n",
        "    train = merge_returnerfeats(train).sort_values('x')\n",
        "\n",
        "    game_id = train.groupby(['GameKey' ,'PlayID'])['x'].first().reset_index()['GameKey'].values\n",
        "    y_mae = train.groupby(['GameKey','PlayID'])[TARGET_COLUMNS].first().reset_index()[TARGET_COLUMNS].values\n",
        "    y_mae = np.where(y_mae < -30, -30, y_mae)\n",
        "    y_mae = np.where(y_mae > 50, 50, y_mae)\n",
        "    y_crps = np.zeros((y_mae.shape[0], 199))\n",
        "    for idx, target in enumerate(list(y_mae)):\n",
        "        y_crps[idx][99 + int(target):] = 1\n",
        "\n",
        "    n_train = len(train.groupby(['GameKey','PlayID']))\n",
        "\n",
        "    n_df = len(train)\n",
        "    x, x_aug = create_features(train)\n",
        "    x = np.concatenate([x, x_aug], axis=0)\n",
        "\n",
        "    x, sc_mean, sc_std = scaling(x)\n",
        "    x_aug = x[n_train:]\n",
        "    x = x[:n_train]\n",
        "    #LOGGER.info(len(x), len(x_aug))\n",
        "\n",
        "with timer('split data'):\n",
        "#     x_2017, y_crps_2017, y_mae_2017 = x[season==2017], y_crps[season==2017], y_mae[season==2017]\n",
        "    x_usage, y_crps_usage, y_mae_usage = x, y_crps, y_mae\n",
        "#     x_aug_2017 = x_aug[season==2017]\n",
        "    x_aug_usage = x_aug\n",
        "    folds = GroupKFold(n_splits=N_SPLITS).split(y_mae_usage, y_mae_usage, groups=game_id)\n",
        "\n",
        "with timer('train'):\n",
        "    scores = []\n",
        "    for n_fold, (train_idx, val_idx) in enumerate(folds):\n",
        "        with timer('create model'):\n",
        "            x_train, y_train, y_train_mae = x_usage[train_idx], y_crps_usage[train_idx], y_mae_usage[train_idx]\n",
        "            x_val, y_val, y_val_mae = x_usage[val_idx], y_crps_usage[val_idx], y_mae_usage[val_idx]\n",
        "            x_aug_train, x_aug_val = x_aug_usage[train_idx], x_aug_usage[val_idx]\n",
        "\n",
        "            # add 2017 data\n",
        "#             x_train = np.concatenate([x_train, x_2017, x_aug_2017, x_aug_train], axis=0)\n",
        "#             y_train = np.concatenate([y_train, y_crps_2017, y_crps_2017, y_train], axis=0)\n",
        "\n",
        "            train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
        "            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "            val_dataset = TensorDataset(torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "            val_dataset_aug = TensorDataset(torch.tensor(x_aug_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
        "            val_loader_aug = DataLoader(val_dataset_aug, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "            del train_dataset, val_dataset\n",
        "            gc.collect()\n",
        "\n",
        "            model = CnnModel(num_classes=N_CLASSES)\n",
        "            #model.to(device)\n",
        "\n",
        "            num_steps = len(x_train) // BATCH_SIZE\n",
        "            #criterion = torch.nn.CrossEntropyLoss()\n",
        "            criterion = CRPSLoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "            #scheduler = OneCycleLR(optimizer, num_steps=num_steps, lr_range=(0.0005, 0.001))\n",
        "            scheduler = None\n",
        "\n",
        "        with timer('train fold{}'.format(n_fold)):\n",
        "            best_score = 999\n",
        "            best_epoch = 0\n",
        "            y_pred = np.zeros_like(y_crps)\n",
        "            for epoch in range(1, epochs + 1):\n",
        "                seed_torch(SEED + epoch)\n",
        "\n",
        "                LOGGER.info(\"Starting {} epoch...\".format(epoch))\n",
        "                tr_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, scheduler=scheduler)\n",
        "                LOGGER.info('Mean train loss: {}'.format(round(tr_loss, 5)))\n",
        "\n",
        "                val_pred, y_true, val_loss = validate(model, val_loader, criterion, device)\n",
        "                if TTA:\n",
        "                    val_pred_aug, _, val_loss_aug = validate(model, val_loader_aug, criterion, device)\n",
        "                    LOGGER.info('valid loss: {} valid loss aug: {}'.format(round(val_loss, 5), round(val_loss_aug, 5)))\n",
        "                    val_loss = (val_loss + val_loss_aug) / 2\n",
        "                    val_pred = (val_pred + val_pred_aug) / 2\n",
        "                score = crps(y_val, val_pred)\n",
        "                LOGGER.info('Mean valid loss: {} score: {}'.format(round(val_loss, 5), round(score, 5)))\n",
        "                if score < best_score:\n",
        "                    best_score = score\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), '{}_fold{}.pth'.format(EXP_ID, n_fold))\n",
        "                    y_pred[val_idx] = val_pred\n",
        "\n",
        "            scores.append(best_score)\n",
        "            LOGGER.info(\"best score={} on epoch={} fold={}\".format(best_score, best_epoch, n_fold))\n",
        "    LOGGER.info(\"score avg={}, score fold0={}, score fold1={}, score fold2={}, score fold3={}, score fold4={}\".format(\n",
        "        np.mean(scores), scores[0], scores[1], scores[2], scores[3], scores[4]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:29:08.584595Z",
          "iopub.execute_input": "2022-10-04T12:29:08.585530Z",
          "iopub.status.idle": "2022-10-04T12:48:08.168340Z",
          "shell.execute_reply.started": "2022-10-04T12:29:08.585467Z",
          "shell.execute_reply": "2022-10-04T12:48:08.166836Z"
        },
        "trusted": true,
        "id": "7zxJQ0Vu1io9",
        "outputId": "cb2c4518-afb0-418d-f0d8-99f021de6efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2022-10-04 12:29:09,113 - INFO - logger set up\n2022-10-04 12:29:09,118 - INFO - seed=46230\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n2022-10-04 12:32:10,249 - INFO - [load data] done in 181.13 s\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  if __name__ == \"__main__\":\n2022-10-04 12:32:11,835 - INFO - [create features] done in 1.58 s\n2022-10-04 12:32:11,837 - INFO - [split data] done in 0.0 s\n2022-10-04 12:32:12,013 - INFO - [create model] done in 0.17 s\n2022-10-04 12:32:12,016 - INFO - Starting 1 epoch...\n2022-10-04 12:32:13,667 - INFO - Mean train loss: 0.04089\n2022-10-04 12:32:15,992 - INFO - valid loss: 0.02657 valid loss aug: 0.02663\n2022-10-04 12:32:15,999 - INFO - Mean valid loss: 0.0266 score: 0.0264\n2022-10-04 12:32:16,008 - INFO - Starting 2 epoch...\n2022-10-04 12:32:17,450 - INFO - Mean train loss: 0.02523\n2022-10-04 12:32:19,802 - INFO - valid loss: 0.02605 valid loss aug: 0.02612\n2022-10-04 12:32:19,805 - INFO - Mean valid loss: 0.02608 score: 0.02598\n2022-10-04 12:32:19,813 - INFO - Starting 3 epoch...\n2022-10-04 12:32:21,263 - INFO - Mean train loss: 0.02519\n2022-10-04 12:32:23,643 - INFO - valid loss: 0.02588 valid loss aug: 0.02596\n2022-10-04 12:32:23,646 - INFO - Mean valid loss: 0.02592 score: 0.0258\n2022-10-04 12:32:23,654 - INFO - Starting 4 epoch...\n2022-10-04 12:32:25,116 - INFO - Mean train loss: 0.02499\n2022-10-04 12:32:27,534 - INFO - valid loss: 0.02594 valid loss aug: 0.02605\n2022-10-04 12:32:27,537 - INFO - Mean valid loss: 0.02599 score: 0.02587\n2022-10-04 12:32:27,539 - INFO - Starting 5 epoch...\n2022-10-04 12:32:29,103 - INFO - Mean train loss: 0.02477\n2022-10-04 12:32:31,452 - INFO - valid loss: 0.02585 valid loss aug: 0.02594\n2022-10-04 12:32:31,454 - INFO - Mean valid loss: 0.02589 score: 0.02575\n2022-10-04 12:32:31,461 - INFO - Starting 6 epoch...\n2022-10-04 12:32:32,905 - INFO - Mean train loss: 0.02492\n2022-10-04 12:32:35,271 - INFO - valid loss: 0.0256 valid loss aug: 0.02553\n2022-10-04 12:32:35,274 - INFO - Mean valid loss: 0.02557 score: 0.0255\n2022-10-04 12:32:35,281 - INFO - Starting 7 epoch...\n2022-10-04 12:32:36,715 - INFO - Mean train loss: 0.02438\n2022-10-04 12:32:39,055 - INFO - valid loss: 0.02559 valid loss aug: 0.0255\n2022-10-04 12:32:39,058 - INFO - Mean valid loss: 0.02554 score: 0.02547\n2022-10-04 12:32:39,065 - INFO - Starting 8 epoch...\n2022-10-04 12:32:40,502 - INFO - Mean train loss: 0.02463\n2022-10-04 12:32:42,829 - INFO - valid loss: 0.02577 valid loss aug: 0.02531\n2022-10-04 12:32:42,832 - INFO - Mean valid loss: 0.02554 score: 0.02536\n2022-10-04 12:32:42,839 - INFO - Starting 9 epoch...\n2022-10-04 12:32:44,306 - INFO - Mean train loss: 0.0244\n2022-10-04 12:32:46,653 - INFO - valid loss: 0.02547 valid loss aug: 0.02562\n2022-10-04 12:32:46,656 - INFO - Mean valid loss: 0.02554 score: 0.02537\n2022-10-04 12:32:46,659 - INFO - Starting 10 epoch...\n2022-10-04 12:32:48,110 - INFO - Mean train loss: 0.02536\n2022-10-04 12:32:50,457 - INFO - valid loss: 0.02544 valid loss aug: 0.02524\n2022-10-04 12:32:50,460 - INFO - Mean valid loss: 0.02534 score: 0.02517\n2022-10-04 12:32:50,467 - INFO - Starting 11 epoch...\n2022-10-04 12:32:51,910 - INFO - Mean train loss: 0.02422\n2022-10-04 12:32:54,266 - INFO - valid loss: 0.02559 valid loss aug: 0.0254\n2022-10-04 12:32:54,269 - INFO - Mean valid loss: 0.02549 score: 0.02538\n2022-10-04 12:32:54,271 - INFO - Starting 12 epoch...\n2022-10-04 12:32:55,720 - INFO - Mean train loss: 0.02433\n2022-10-04 12:32:58,062 - INFO - valid loss: 0.02549 valid loss aug: 0.0254\n2022-10-04 12:32:58,065 - INFO - Mean valid loss: 0.02544 score: 0.02532\n2022-10-04 12:32:58,068 - INFO - Starting 13 epoch...\n2022-10-04 12:32:59,865 - INFO - Mean train loss: 0.02421\n2022-10-04 12:33:02,199 - INFO - valid loss: 0.02542 valid loss aug: 0.02495\n2022-10-04 12:33:02,202 - INFO - Mean valid loss: 0.02518 score: 0.02504\n2022-10-04 12:33:02,210 - INFO - Starting 14 epoch...\n2022-10-04 12:33:03,683 - INFO - Mean train loss: 0.02383\n2022-10-04 12:33:06,002 - INFO - valid loss: 0.02569 valid loss aug: 0.02525\n2022-10-04 12:33:06,005 - INFO - Mean valid loss: 0.02547 score: 0.0252\n2022-10-04 12:33:06,007 - INFO - Starting 15 epoch...\n2022-10-04 12:33:07,506 - INFO - Mean train loss: 0.02436\n2022-10-04 12:33:09,840 - INFO - valid loss: 0.02543 valid loss aug: 0.02505\n2022-10-04 12:33:09,844 - INFO - Mean valid loss: 0.02524 score: 0.02493\n2022-10-04 12:33:09,851 - INFO - Starting 16 epoch...\n2022-10-04 12:33:11,292 - INFO - Mean train loss: 0.02422\n2022-10-04 12:33:13,619 - INFO - valid loss: 0.0256 valid loss aug: 0.02557\n2022-10-04 12:33:13,622 - INFO - Mean valid loss: 0.02559 score: 0.02536\n2022-10-04 12:33:13,624 - INFO - Starting 17 epoch...\n2022-10-04 12:33:15,048 - INFO - Mean train loss: 0.02383\n2022-10-04 12:33:17,373 - INFO - valid loss: 0.02542 valid loss aug: 0.02511\n2022-10-04 12:33:17,376 - INFO - Mean valid loss: 0.02527 score: 0.02503\n2022-10-04 12:33:17,379 - INFO - Starting 18 epoch...\n2022-10-04 12:33:18,847 - INFO - Mean train loss: 0.02371\n2022-10-04 12:33:21,150 - INFO - valid loss: 0.02547 valid loss aug: 0.02537\n2022-10-04 12:33:21,155 - INFO - Mean valid loss: 0.02542 score: 0.02504\n2022-10-04 12:33:21,157 - INFO - Starting 19 epoch...\n2022-10-04 12:33:22,595 - INFO - Mean train loss: 0.02395\n2022-10-04 12:33:24,937 - INFO - valid loss: 0.02527 valid loss aug: 0.02511\n2022-10-04 12:33:24,940 - INFO - Mean valid loss: 0.02519 score: 0.02486\n2022-10-04 12:33:24,948 - INFO - Starting 20 epoch...\n2022-10-04 12:33:26,393 - INFO - Mean train loss: 0.02371\n2022-10-04 12:33:28,722 - INFO - valid loss: 0.02552 valid loss aug: 0.02551\n2022-10-04 12:33:28,724 - INFO - Mean valid loss: 0.02551 score: 0.02532\n2022-10-04 12:33:28,727 - INFO - Starting 21 epoch...\n2022-10-04 12:33:30,369 - INFO - Mean train loss: 0.02397\n2022-10-04 12:33:32,731 - INFO - valid loss: 0.02535 valid loss aug: 0.0253\n2022-10-04 12:33:32,734 - INFO - Mean valid loss: 0.02532 score: 0.02509\n2022-10-04 12:33:32,736 - INFO - Starting 22 epoch...\n2022-10-04 12:33:34,202 - INFO - Mean train loss: 0.02358\n2022-10-04 12:33:36,579 - INFO - valid loss: 0.02559 valid loss aug: 0.02557\n2022-10-04 12:33:36,582 - INFO - Mean valid loss: 0.02558 score: 0.02533\n2022-10-04 12:33:36,585 - INFO - Starting 23 epoch...\n2022-10-04 12:33:38,056 - INFO - Mean train loss: 0.02372\n2022-10-04 12:33:40,522 - INFO - valid loss: 0.02541 valid loss aug: 0.02519\n2022-10-04 12:33:40,525 - INFO - Mean valid loss: 0.0253 score: 0.025\n2022-10-04 12:33:40,528 - INFO - Starting 24 epoch...\n2022-10-04 12:33:42,035 - INFO - Mean train loss: 0.02405\n2022-10-04 12:33:44,360 - INFO - valid loss: 0.02535 valid loss aug: 0.02497\n2022-10-04 12:33:44,363 - INFO - Mean valid loss: 0.02516 score: 0.0249\n2022-10-04 12:33:44,366 - INFO - Starting 25 epoch...\n2022-10-04 12:33:45,845 - INFO - Mean train loss: 0.02349\n2022-10-04 12:33:48,178 - INFO - valid loss: 0.02524 valid loss aug: 0.02502\n2022-10-04 12:33:48,181 - INFO - Mean valid loss: 0.02513 score: 0.02482\n2022-10-04 12:33:48,188 - INFO - Starting 26 epoch...\n2022-10-04 12:33:49,650 - INFO - Mean train loss: 0.02383\n2022-10-04 12:33:51,980 - INFO - valid loss: 0.02564 valid loss aug: 0.02549\n2022-10-04 12:33:51,983 - INFO - Mean valid loss: 0.02556 score: 0.02529\n2022-10-04 12:33:51,986 - INFO - Starting 27 epoch...\n2022-10-04 12:33:53,505 - INFO - Mean train loss: 0.02368\n2022-10-04 12:33:55,848 - INFO - valid loss: 0.02516 valid loss aug: 0.02505\n2022-10-04 12:33:55,851 - INFO - Mean valid loss: 0.02511 score: 0.02493\n2022-10-04 12:33:55,853 - INFO - Starting 28 epoch...\n2022-10-04 12:33:57,298 - INFO - Mean train loss: 0.02324\n2022-10-04 12:33:59,663 - INFO - valid loss: 0.02572 valid loss aug: 0.02549\n2022-10-04 12:33:59,665 - INFO - Mean valid loss: 0.02561 score: 0.02501\n2022-10-04 12:33:59,668 - INFO - Starting 29 epoch...\n2022-10-04 12:34:01,140 - INFO - Mean train loss: 0.02356\n2022-10-04 12:34:03,612 - INFO - valid loss: 0.02565 valid loss aug: 0.02526\n2022-10-04 12:34:03,615 - INFO - Mean valid loss: 0.02545 score: 0.02508\n2022-10-04 12:34:03,618 - INFO - Starting 30 epoch...\n2022-10-04 12:34:05,118 - INFO - Mean train loss: 0.02328\n2022-10-04 12:34:07,478 - INFO - valid loss: 0.0259 valid loss aug: 0.02549\n2022-10-04 12:34:07,481 - INFO - Mean valid loss: 0.0257 score: 0.02531\n2022-10-04 12:34:07,484 - INFO - Starting 31 epoch...\n2022-10-04 12:34:08,966 - INFO - Mean train loss: 0.02323\n2022-10-04 12:34:11,313 - INFO - valid loss: 0.02584 valid loss aug: 0.02519\n2022-10-04 12:34:11,316 - INFO - Mean valid loss: 0.02551 score: 0.0252\n2022-10-04 12:34:11,320 - INFO - Starting 32 epoch...\n2022-10-04 12:34:12,777 - INFO - Mean train loss: 0.02465\n2022-10-04 12:34:15,131 - INFO - valid loss: 0.02568 valid loss aug: 0.02517\n2022-10-04 12:34:15,135 - INFO - Mean valid loss: 0.02542 score: 0.02507\n2022-10-04 12:34:15,138 - INFO - Starting 33 epoch...\n2022-10-04 12:34:16,579 - INFO - Mean train loss: 0.02435\n2022-10-04 12:34:18,913 - INFO - valid loss: 0.02553 valid loss aug: 0.02536\n2022-10-04 12:34:18,915 - INFO - Mean valid loss: 0.02545 score: 0.02507\n2022-10-04 12:34:18,918 - INFO - Starting 34 epoch...\n2022-10-04 12:34:20,347 - INFO - Mean train loss: 0.02396\n2022-10-04 12:34:22,657 - INFO - valid loss: 0.02557 valid loss aug: 0.02572\n2022-10-04 12:34:22,660 - INFO - Mean valid loss: 0.02564 score: 0.02526\n2022-10-04 12:34:22,664 - INFO - Starting 35 epoch...\n2022-10-04 12:34:24,158 - INFO - Mean train loss: 0.02387\n2022-10-04 12:34:26,505 - INFO - valid loss: 0.02547 valid loss aug: 0.02513\n2022-10-04 12:34:26,508 - INFO - Mean valid loss: 0.0253 score: 0.02499\n2022-10-04 12:34:26,511 - INFO - Starting 36 epoch...\n2022-10-04 12:34:27,972 - INFO - Mean train loss: 0.02327\n2022-10-04 12:34:30,280 - INFO - valid loss: 0.02541 valid loss aug: 0.02526\n2022-10-04 12:34:30,282 - INFO - Mean valid loss: 0.02534 score: 0.025\n2022-10-04 12:34:30,285 - INFO - Starting 37 epoch...\n2022-10-04 12:34:31,718 - INFO - Mean train loss: 0.02291\n2022-10-04 12:34:34,130 - INFO - valid loss: 0.02567 valid loss aug: 0.02526\n2022-10-04 12:34:34,133 - INFO - Mean valid loss: 0.02546 score: 0.02507\n2022-10-04 12:34:34,136 - INFO - Starting 38 epoch...\n2022-10-04 12:34:35,638 - INFO - Mean train loss: 0.02329\n2022-10-04 12:34:37,975 - INFO - valid loss: 0.02554 valid loss aug: 0.0255\n2022-10-04 12:34:37,979 - INFO - Mean valid loss: 0.02552 score: 0.02514\n2022-10-04 12:34:37,982 - INFO - Starting 39 epoch...\n2022-10-04 12:34:39,442 - INFO - Mean train loss: 0.02376\n2022-10-04 12:34:41,772 - INFO - valid loss: 0.02575 valid loss aug: 0.02535\n2022-10-04 12:34:41,775 - INFO - Mean valid loss: 0.02555 score: 0.02516\n2022-10-04 12:34:41,778 - INFO - Starting 40 epoch...\n2022-10-04 12:34:43,216 - INFO - Mean train loss: 0.02306\n2022-10-04 12:34:45,549 - INFO - valid loss: 0.02555 valid loss aug: 0.02515\n2022-10-04 12:34:45,553 - INFO - Mean valid loss: 0.02535 score: 0.02491\n2022-10-04 12:34:45,555 - INFO - Starting 41 epoch...\n2022-10-04 12:34:47,037 - INFO - Mean train loss: 0.02261\n2022-10-04 12:34:49,368 - INFO - valid loss: 0.02601 valid loss aug: 0.0256\n2022-10-04 12:34:49,371 - INFO - Mean valid loss: 0.02581 score: 0.02529\n2022-10-04 12:34:49,374 - INFO - Starting 42 epoch...\n2022-10-04 12:34:50,813 - INFO - Mean train loss: 0.02287\n2022-10-04 12:34:53,130 - INFO - valid loss: 0.02542 valid loss aug: 0.02532\n2022-10-04 12:34:53,133 - INFO - Mean valid loss: 0.02537 score: 0.02499\n2022-10-04 12:34:53,135 - INFO - Starting 43 epoch...\n2022-10-04 12:34:54,571 - INFO - Mean train loss: 0.02332\n2022-10-04 12:34:56,912 - INFO - valid loss: 0.02569 valid loss aug: 0.0255\n2022-10-04 12:34:56,916 - INFO - Mean valid loss: 0.02559 score: 0.02503\n2022-10-04 12:34:56,918 - INFO - Starting 44 epoch...\n2022-10-04 12:34:58,384 - INFO - Mean train loss: 0.02298\n2022-10-04 12:35:00,692 - INFO - valid loss: 0.02579 valid loss aug: 0.02555\n2022-10-04 12:35:00,695 - INFO - Mean valid loss: 0.02567 score: 0.02517\n2022-10-04 12:35:00,697 - INFO - Starting 45 epoch...\n2022-10-04 12:35:02,137 - INFO - Mean train loss: 0.02269\n2022-10-04 12:35:04,533 - INFO - valid loss: 0.02579 valid loss aug: 0.02552\n2022-10-04 12:35:04,538 - INFO - Mean valid loss: 0.02566 score: 0.02512\n2022-10-04 12:35:04,542 - INFO - Starting 46 epoch...\n2022-10-04 12:35:05,989 - INFO - Mean train loss: 0.02399\n2022-10-04 12:35:08,326 - INFO - valid loss: 0.02616 valid loss aug: 0.02555\n2022-10-04 12:35:08,330 - INFO - Mean valid loss: 0.02586 score: 0.02513\n2022-10-04 12:35:08,333 - INFO - Starting 47 epoch...\n2022-10-04 12:35:09,786 - INFO - Mean train loss: 0.0233\n2022-10-04 12:35:12,318 - INFO - valid loss: 0.02561 valid loss aug: 0.02554\n2022-10-04 12:35:12,320 - INFO - Mean valid loss: 0.02557 score: 0.02524\n2022-10-04 12:35:12,322 - INFO - Starting 48 epoch...\n2022-10-04 12:35:14,282 - INFO - Mean train loss: 0.02264\n2022-10-04 12:35:16,622 - INFO - valid loss: 0.02572 valid loss aug: 0.02543\n2022-10-04 12:35:16,625 - INFO - Mean valid loss: 0.02558 score: 0.02513\n2022-10-04 12:35:16,628 - INFO - Starting 49 epoch...\n2022-10-04 12:35:18,095 - INFO - Mean train loss: 0.02221\n2022-10-04 12:35:20,429 - INFO - valid loss: 0.02581 valid loss aug: 0.02559\n2022-10-04 12:35:20,432 - INFO - Mean valid loss: 0.0257 score: 0.02518\n2022-10-04 12:35:20,435 - INFO - Starting 50 epoch...\n2022-10-04 12:35:21,873 - INFO - Mean train loss: 0.02392\n2022-10-04 12:35:24,237 - INFO - valid loss: 0.02599 valid loss aug: 0.02597\n2022-10-04 12:35:24,240 - INFO - Mean valid loss: 0.02598 score: 0.02531\n2022-10-04 12:35:24,241 - INFO - best score=0.024824 on epoch=25 fold=0\n2022-10-04 12:35:24,242 - INFO - [train fold0] done in 192.23 s\n2022-10-04 12:35:24,393 - INFO - [create model] done in 0.15 s\n2022-10-04 12:35:24,396 - INFO - Starting 1 epoch...\n2022-10-04 12:35:25,833 - INFO - Mean train loss: 0.04023\n2022-10-04 12:35:28,157 - INFO - valid loss: 0.02621 valid loss aug: 0.02632\n2022-10-04 12:35:28,159 - INFO - Mean valid loss: 0.02626 score: 0.02621\n2022-10-04 12:35:28,166 - INFO - Starting 2 epoch...\n2022-10-04 12:35:29,610 - INFO - Mean train loss: 0.02612\n2022-10-04 12:35:31,938 - INFO - valid loss: 0.02569 valid loss aug: 0.0257\n2022-10-04 12:35:31,941 - INFO - Mean valid loss: 0.0257 score: 0.02584\n2022-10-04 12:35:31,948 - INFO - Starting 3 epoch...\n2022-10-04 12:35:33,394 - INFO - Mean train loss: 0.02487\n2022-10-04 12:35:35,802 - INFO - valid loss: 0.02558 valid loss aug: 0.02576\n2022-10-04 12:35:35,805 - INFO - Mean valid loss: 0.02567 score: 0.02571\n2022-10-04 12:35:35,813 - INFO - Starting 4 epoch...\n2022-10-04 12:35:37,249 - INFO - Mean train loss: 0.02497\n2022-10-04 12:35:39,578 - INFO - valid loss: 0.02559 valid loss aug: 0.0254\n2022-10-04 12:35:39,581 - INFO - Mean valid loss: 0.0255 score: 0.02552\n2022-10-04 12:35:39,588 - INFO - Starting 5 epoch...\n2022-10-04 12:35:41,095 - INFO - Mean train loss: 0.02463\n2022-10-04 12:35:43,468 - INFO - valid loss: 0.0258 valid loss aug: 0.02563\n2022-10-04 12:35:43,471 - INFO - Mean valid loss: 0.02571 score: 0.02556\n2022-10-04 12:35:43,473 - INFO - Starting 6 epoch...\n2022-10-04 12:35:44,942 - INFO - Mean train loss: 0.02452\n2022-10-04 12:35:47,299 - INFO - valid loss: 0.02568 valid loss aug: 0.02567\n2022-10-04 12:35:47,302 - INFO - Mean valid loss: 0.02567 score: 0.02574\n2022-10-04 12:35:47,304 - INFO - Starting 7 epoch...\n2022-10-04 12:35:48,771 - INFO - Mean train loss: 0.02441\n2022-10-04 12:35:51,086 - INFO - valid loss: 0.02537 valid loss aug: 0.02543\n2022-10-04 12:35:51,089 - INFO - Mean valid loss: 0.0254 score: 0.02533\n2022-10-04 12:35:51,098 - INFO - Starting 8 epoch...\n2022-10-04 12:35:52,549 - INFO - Mean train loss: 0.02433\n2022-10-04 12:35:54,866 - INFO - valid loss: 0.0257 valid loss aug: 0.02569\n2022-10-04 12:35:54,869 - INFO - Mean valid loss: 0.0257 score: 0.02566\n2022-10-04 12:35:54,872 - INFO - Starting 9 epoch...\n2022-10-04 12:35:56,298 - INFO - Mean train loss: 0.02422\n2022-10-04 12:35:58,620 - INFO - valid loss: 0.02534 valid loss aug: 0.0254\n2022-10-04 12:35:58,623 - INFO - Mean valid loss: 0.02537 score: 0.02544\n2022-10-04 12:35:58,626 - INFO - Starting 10 epoch...\n2022-10-04 12:36:00,062 - INFO - Mean train loss: 0.02392\n2022-10-04 12:36:02,390 - INFO - valid loss: 0.02542 valid loss aug: 0.02563\n2022-10-04 12:36:02,393 - INFO - Mean valid loss: 0.02552 score: 0.02556\n2022-10-04 12:36:02,396 - INFO - Starting 11 epoch...\n2022-10-04 12:36:03,869 - INFO - Mean train loss: 0.02397\n2022-10-04 12:36:06,205 - INFO - valid loss: 0.02554 valid loss aug: 0.02558\n2022-10-04 12:36:06,210 - INFO - Mean valid loss: 0.02556 score: 0.02557\n2022-10-04 12:36:06,215 - INFO - Starting 12 epoch...\n2022-10-04 12:36:07,885 - INFO - Mean train loss: 0.02392\n2022-10-04 12:36:10,200 - INFO - valid loss: 0.0259 valid loss aug: 0.02612\n2022-10-04 12:36:10,202 - INFO - Mean valid loss: 0.02601 score: 0.0259\n2022-10-04 12:36:10,205 - INFO - Starting 13 epoch...\n2022-10-04 12:36:11,633 - INFO - Mean train loss: 0.02632\n2022-10-04 12:36:14,000 - INFO - valid loss: 0.02563 valid loss aug: 0.02579\n2022-10-04 12:36:14,003 - INFO - Mean valid loss: 0.02571 score: 0.02571\n2022-10-04 12:36:14,006 - INFO - Starting 14 epoch...\n2022-10-04 12:36:15,433 - INFO - Mean train loss: 0.02506\n2022-10-04 12:36:17,737 - INFO - valid loss: 0.02573 valid loss aug: 0.02557\n2022-10-04 12:36:17,740 - INFO - Mean valid loss: 0.02565 score: 0.02555\n2022-10-04 12:36:17,743 - INFO - Starting 15 epoch...\n2022-10-04 12:36:19,166 - INFO - Mean train loss: 0.02374\n2022-10-04 12:36:21,483 - INFO - valid loss: 0.02582 valid loss aug: 0.02574\n2022-10-04 12:36:21,486 - INFO - Mean valid loss: 0.02578 score: 0.02584\n2022-10-04 12:36:21,489 - INFO - Starting 16 epoch...\n2022-10-04 12:36:22,911 - INFO - Mean train loss: 0.02407\n2022-10-04 12:36:25,251 - INFO - valid loss: 0.0259 valid loss aug: 0.02602\n2022-10-04 12:36:25,253 - INFO - Mean valid loss: 0.02596 score: 0.02577\n2022-10-04 12:36:25,256 - INFO - Starting 17 epoch...\n2022-10-04 12:36:26,704 - INFO - Mean train loss: 0.02418\n2022-10-04 12:36:29,027 - INFO - valid loss: 0.02552 valid loss aug: 0.02567\n2022-10-04 12:36:29,030 - INFO - Mean valid loss: 0.02559 score: 0.02561\n2022-10-04 12:36:29,033 - INFO - Starting 18 epoch...\n2022-10-04 12:36:30,430 - INFO - Mean train loss: 0.02362\n2022-10-04 12:36:32,700 - INFO - valid loss: 0.02533 valid loss aug: 0.02552\n2022-10-04 12:36:32,703 - INFO - Mean valid loss: 0.02543 score: 0.02549\n2022-10-04 12:36:32,706 - INFO - Starting 19 epoch...\n2022-10-04 12:36:34,131 - INFO - Mean train loss: 0.02396\n2022-10-04 12:36:36,458 - INFO - valid loss: 0.02552 valid loss aug: 0.02572\n2022-10-04 12:36:36,461 - INFO - Mean valid loss: 0.02562 score: 0.02556\n2022-10-04 12:36:36,463 - INFO - Starting 20 epoch...\n2022-10-04 12:36:38,111 - INFO - Mean train loss: 0.02366\n2022-10-04 12:36:40,440 - INFO - valid loss: 0.02566 valid loss aug: 0.02568\n2022-10-04 12:36:40,442 - INFO - Mean valid loss: 0.02567 score: 0.02566\n2022-10-04 12:36:40,445 - INFO - Starting 21 epoch...\n2022-10-04 12:36:41,875 - INFO - Mean train loss: 0.02389\n2022-10-04 12:36:44,156 - INFO - valid loss: 0.02557 valid loss aug: 0.02566\n2022-10-04 12:36:44,159 - INFO - Mean valid loss: 0.02561 score: 0.02553\n2022-10-04 12:36:44,161 - INFO - Starting 22 epoch...\n2022-10-04 12:36:45,689 - INFO - Mean train loss: 0.02353\n2022-10-04 12:36:48,002 - INFO - valid loss: 0.02546 valid loss aug: 0.02548\n2022-10-04 12:36:48,005 - INFO - Mean valid loss: 0.02547 score: 0.02544\n2022-10-04 12:36:48,008 - INFO - Starting 23 epoch...\n2022-10-04 12:36:49,428 - INFO - Mean train loss: 0.02462\n2022-10-04 12:36:51,737 - INFO - valid loss: 0.02556 valid loss aug: 0.02554\n2022-10-04 12:36:51,740 - INFO - Mean valid loss: 0.02555 score: 0.02548\n2022-10-04 12:36:51,742 - INFO - Starting 24 epoch...\n2022-10-04 12:36:53,166 - INFO - Mean train loss: 0.02375\n2022-10-04 12:36:55,484 - INFO - valid loss: 0.02553 valid loss aug: 0.02547\n2022-10-04 12:36:55,487 - INFO - Mean valid loss: 0.0255 score: 0.0255\n2022-10-04 12:36:55,489 - INFO - Starting 25 epoch...\n2022-10-04 12:36:56,929 - INFO - Mean train loss: 0.02358\n2022-10-04 12:36:59,283 - INFO - valid loss: 0.02557 valid loss aug: 0.02567\n2022-10-04 12:36:59,286 - INFO - Mean valid loss: 0.02562 score: 0.02559\n2022-10-04 12:36:59,288 - INFO - Starting 26 epoch...\n2022-10-04 12:37:00,706 - INFO - Mean train loss: 0.02376\n2022-10-04 12:37:03,039 - INFO - valid loss: 0.02539 valid loss aug: 0.02566\n2022-10-04 12:37:03,042 - INFO - Mean valid loss: 0.02552 score: 0.02553\n2022-10-04 12:37:03,045 - INFO - Starting 27 epoch...\n2022-10-04 12:37:04,487 - INFO - Mean train loss: 0.02332\n2022-10-04 12:37:06,797 - INFO - valid loss: 0.02598 valid loss aug: 0.02585\n2022-10-04 12:37:06,800 - INFO - Mean valid loss: 0.02592 score: 0.02568\n2022-10-04 12:37:06,803 - INFO - Starting 28 epoch...\n2022-10-04 12:37:08,231 - INFO - Mean train loss: 0.02349\n2022-10-04 12:37:10,644 - INFO - valid loss: 0.02592 valid loss aug: 0.02578\n2022-10-04 12:37:10,646 - INFO - Mean valid loss: 0.02585 score: 0.02556\n2022-10-04 12:37:10,649 - INFO - Starting 29 epoch...\n2022-10-04 12:37:12,057 - INFO - Mean train loss: 0.02322\n2022-10-04 12:37:14,331 - INFO - valid loss: 0.02564 valid loss aug: 0.02557\n2022-10-04 12:37:14,334 - INFO - Mean valid loss: 0.02561 score: 0.02562\n2022-10-04 12:37:14,336 - INFO - Starting 30 epoch...\n2022-10-04 12:37:15,758 - INFO - Mean train loss: 0.02288\n2022-10-04 12:37:18,048 - INFO - valid loss: 0.02556 valid loss aug: 0.02563\n2022-10-04 12:37:18,051 - INFO - Mean valid loss: 0.0256 score: 0.0255\n2022-10-04 12:37:18,053 - INFO - Starting 31 epoch...\n2022-10-04 12:37:19,443 - INFO - Mean train loss: 0.02467\n2022-10-04 12:37:21,738 - INFO - valid loss: 0.02604 valid loss aug: 0.02579\n2022-10-04 12:37:21,741 - INFO - Mean valid loss: 0.02592 score: 0.02569\n2022-10-04 12:37:21,743 - INFO - Starting 32 epoch...\n2022-10-04 12:37:23,170 - INFO - Mean train loss: 0.02374\n2022-10-04 12:37:25,471 - INFO - valid loss: 0.02575 valid loss aug: 0.02579\n2022-10-04 12:37:25,474 - INFO - Mean valid loss: 0.02577 score: 0.02557\n2022-10-04 12:37:25,477 - INFO - Starting 33 epoch...\n2022-10-04 12:37:26,919 - INFO - Mean train loss: 0.02327\n2022-10-04 12:37:29,245 - INFO - valid loss: 0.0257 valid loss aug: 0.02585\n2022-10-04 12:37:29,248 - INFO - Mean valid loss: 0.02578 score: 0.02558\n2022-10-04 12:37:29,250 - INFO - Starting 34 epoch...\n2022-10-04 12:37:30,694 - INFO - Mean train loss: 0.02308\n2022-10-04 12:37:33,041 - INFO - valid loss: 0.02575 valid loss aug: 0.02589\n2022-10-04 12:37:33,044 - INFO - Mean valid loss: 0.02582 score: 0.02572\n2022-10-04 12:37:33,046 - INFO - Starting 35 epoch...\n2022-10-04 12:37:34,489 - INFO - Mean train loss: 0.02508\n2022-10-04 12:37:36,781 - INFO - valid loss: 0.02596 valid loss aug: 0.02601\n2022-10-04 12:37:36,784 - INFO - Mean valid loss: 0.02599 score: 0.02571\n2022-10-04 12:37:36,786 - INFO - Starting 36 epoch...\n2022-10-04 12:37:38,206 - INFO - Mean train loss: 0.02369\n2022-10-04 12:37:40,577 - INFO - valid loss: 0.02569 valid loss aug: 0.02564\n2022-10-04 12:37:40,580 - INFO - Mean valid loss: 0.02567 score: 0.02566\n2022-10-04 12:37:40,584 - INFO - Starting 37 epoch...\n2022-10-04 12:37:42,155 - INFO - Mean train loss: 0.02344\n2022-10-04 12:37:44,506 - INFO - valid loss: 0.02579 valid loss aug: 0.026\n2022-10-04 12:37:44,509 - INFO - Mean valid loss: 0.0259 score: 0.02571\n2022-10-04 12:37:44,512 - INFO - Starting 38 epoch...\n2022-10-04 12:37:45,989 - INFO - Mean train loss: 0.02463\n2022-10-04 12:37:48,323 - INFO - valid loss: 0.02579 valid loss aug: 0.02575\n2022-10-04 12:37:48,326 - INFO - Mean valid loss: 0.02577 score: 0.02555\n2022-10-04 12:37:48,328 - INFO - Starting 39 epoch...\n2022-10-04 12:37:49,768 - INFO - Mean train loss: 0.02362\n2022-10-04 12:37:52,077 - INFO - valid loss: 0.02565 valid loss aug: 0.02593\n2022-10-04 12:37:52,080 - INFO - Mean valid loss: 0.02579 score: 0.02571\n2022-10-04 12:37:52,083 - INFO - Starting 40 epoch...\n2022-10-04 12:37:53,559 - INFO - Mean train loss: 0.02349\n2022-10-04 12:37:55,890 - INFO - valid loss: 0.02544 valid loss aug: 0.0259\n2022-10-04 12:37:55,893 - INFO - Mean valid loss: 0.02567 score: 0.02563\n2022-10-04 12:37:55,895 - INFO - Starting 41 epoch...\n2022-10-04 12:37:57,335 - INFO - Mean train loss: 0.02324\n2022-10-04 12:37:59,644 - INFO - valid loss: 0.02592 valid loss aug: 0.02603\n2022-10-04 12:37:59,647 - INFO - Mean valid loss: 0.02597 score: 0.0257\n2022-10-04 12:37:59,649 - INFO - Starting 42 epoch...\n2022-10-04 12:38:01,060 - INFO - Mean train loss: 0.02277\n2022-10-04 12:38:03,356 - INFO - valid loss: 0.02572 valid loss aug: 0.02603\n2022-10-04 12:38:03,358 - INFO - Mean valid loss: 0.02587 score: 0.02572\n2022-10-04 12:38:03,361 - INFO - Starting 43 epoch...\n2022-10-04 12:38:04,830 - INFO - Mean train loss: 0.02289\n2022-10-04 12:38:07,137 - INFO - valid loss: 0.02607 valid loss aug: 0.02639\n2022-10-04 12:38:07,140 - INFO - Mean valid loss: 0.02623 score: 0.02594\n2022-10-04 12:38:07,144 - INFO - Starting 44 epoch...\n2022-10-04 12:38:08,568 - INFO - Mean train loss: 0.02474\n2022-10-04 12:38:10,895 - INFO - valid loss: 0.02592 valid loss aug: 0.02604\n2022-10-04 12:38:10,898 - INFO - Mean valid loss: 0.02598 score: 0.02566\n2022-10-04 12:38:10,901 - INFO - Starting 45 epoch...\n2022-10-04 12:38:12,684 - INFO - Mean train loss: 0.02242\n2022-10-04 12:38:15,034 - INFO - valid loss: 0.02573 valid loss aug: 0.02571\n2022-10-04 12:38:15,038 - INFO - Mean valid loss: 0.02572 score: 0.02552\n2022-10-04 12:38:15,040 - INFO - Starting 46 epoch...\n2022-10-04 12:38:16,510 - INFO - Mean train loss: 0.02323\n2022-10-04 12:38:18,914 - INFO - valid loss: 0.02572 valid loss aug: 0.02592\n2022-10-04 12:38:18,917 - INFO - Mean valid loss: 0.02582 score: 0.02566\n2022-10-04 12:38:18,919 - INFO - Starting 47 epoch...\n2022-10-04 12:38:20,363 - INFO - Mean train loss: 0.02287\n2022-10-04 12:38:22,670 - INFO - valid loss: 0.02578 valid loss aug: 0.02621\n2022-10-04 12:38:22,673 - INFO - Mean valid loss: 0.026 score: 0.02578\n2022-10-04 12:38:22,677 - INFO - Starting 48 epoch...\n2022-10-04 12:38:24,107 - INFO - Mean train loss: 0.02251\n2022-10-04 12:38:26,449 - INFO - valid loss: 0.026 valid loss aug: 0.02653\n2022-10-04 12:38:26,452 - INFO - Mean valid loss: 0.02627 score: 0.02596\n2022-10-04 12:38:26,454 - INFO - Starting 49 epoch...\n2022-10-04 12:38:27,879 - INFO - Mean train loss: 0.02228\n2022-10-04 12:38:30,173 - INFO - valid loss: 0.02633 valid loss aug: 0.02664\n2022-10-04 12:38:30,176 - INFO - Mean valid loss: 0.02648 score: 0.02599\n2022-10-04 12:38:30,179 - INFO - Starting 50 epoch...\n2022-10-04 12:38:31,590 - INFO - Mean train loss: 0.02344\n2022-10-04 12:38:33,915 - INFO - valid loss: 0.02646 valid loss aug: 0.02683\n2022-10-04 12:38:33,918 - INFO - Mean valid loss: 0.02665 score: 0.0261\n2022-10-04 12:38:33,919 - INFO - best score=0.02533 on epoch=7 fold=1\n2022-10-04 12:38:33,920 - INFO - [train fold1] done in 189.53 s\n2022-10-04 12:38:34,069 - INFO - [create model] done in 0.15 s\n2022-10-04 12:38:34,072 - INFO - Starting 1 epoch...\n2022-10-04 12:38:35,496 - INFO - Mean train loss: 0.04027\n2022-10-04 12:38:37,800 - INFO - valid loss: 0.02567 valid loss aug: 0.02546\n2022-10-04 12:38:37,803 - INFO - Mean valid loss: 0.02557 score: 0.02541\n2022-10-04 12:38:37,812 - INFO - Starting 2 epoch...\n2022-10-04 12:38:39,237 - INFO - Mean train loss: 0.02534\n2022-10-04 12:38:41,544 - INFO - valid loss: 0.02495 valid loss aug: 0.02528\n2022-10-04 12:38:41,547 - INFO - Mean valid loss: 0.02511 score: 0.02507\n2022-10-04 12:38:41,555 - INFO - Starting 3 epoch...\n2022-10-04 12:38:43,143 - INFO - Mean train loss: 0.02501\n2022-10-04 12:38:45,494 - INFO - valid loss: 0.0251 valid loss aug: 0.02515\n2022-10-04 12:38:45,497 - INFO - Mean valid loss: 0.02513 score: 0.02515\n2022-10-04 12:38:45,500 - INFO - Starting 4 epoch...\n2022-10-04 12:38:46,934 - INFO - Mean train loss: 0.02502\n2022-10-04 12:38:49,254 - INFO - valid loss: 0.02513 valid loss aug: 0.02523\n2022-10-04 12:38:49,257 - INFO - Mean valid loss: 0.02518 score: 0.02497\n2022-10-04 12:38:49,264 - INFO - Starting 5 epoch...\n2022-10-04 12:38:50,681 - INFO - Mean train loss: 0.02525\n2022-10-04 12:38:52,958 - INFO - valid loss: 0.02524 valid loss aug: 0.02532\n2022-10-04 12:38:52,960 - INFO - Mean valid loss: 0.02528 score: 0.02524\n2022-10-04 12:38:52,963 - INFO - Starting 6 epoch...\n2022-10-04 12:38:54,412 - INFO - Mean train loss: 0.02501\n2022-10-04 12:38:56,728 - INFO - valid loss: 0.02479 valid loss aug: 0.02505\n2022-10-04 12:38:56,730 - INFO - Mean valid loss: 0.02492 score: 0.025\n2022-10-04 12:38:56,733 - INFO - Starting 7 epoch...\n2022-10-04 12:38:58,192 - INFO - Mean train loss: 0.02505\n2022-10-04 12:39:00,524 - INFO - valid loss: 0.02519 valid loss aug: 0.02494\n2022-10-04 12:39:00,526 - INFO - Mean valid loss: 0.02507 score: 0.0251\n2022-10-04 12:39:00,529 - INFO - Starting 8 epoch...\n2022-10-04 12:39:01,978 - INFO - Mean train loss: 0.02522\n2022-10-04 12:39:04,290 - INFO - valid loss: 0.02471 valid loss aug: 0.02476\n2022-10-04 12:39:04,293 - INFO - Mean valid loss: 0.02474 score: 0.02483\n2022-10-04 12:39:04,300 - INFO - Starting 9 epoch...\n2022-10-04 12:39:05,743 - INFO - Mean train loss: 0.02453\n2022-10-04 12:39:08,021 - INFO - valid loss: 0.02488 valid loss aug: 0.02509\n2022-10-04 12:39:08,024 - INFO - Mean valid loss: 0.02498 score: 0.02497\n2022-10-04 12:39:08,026 - INFO - Starting 10 epoch...\n2022-10-04 12:39:09,471 - INFO - Mean train loss: 0.02517\n2022-10-04 12:39:11,798 - INFO - valid loss: 0.02463 valid loss aug: 0.0248\n2022-10-04 12:39:11,801 - INFO - Mean valid loss: 0.02472 score: 0.02462\n2022-10-04 12:39:11,809 - INFO - Starting 11 epoch...\n2022-10-04 12:39:13,224 - INFO - Mean train loss: 0.02459\n2022-10-04 12:39:15,634 - INFO - valid loss: 0.02445 valid loss aug: 0.02467\n2022-10-04 12:39:15,638 - INFO - Mean valid loss: 0.02456 score: 0.02465\n2022-10-04 12:39:15,641 - INFO - Starting 12 epoch...\n2022-10-04 12:39:17,121 - INFO - Mean train loss: 0.02423\n2022-10-04 12:39:19,479 - INFO - valid loss: 0.02455 valid loss aug: 0.02485\n2022-10-04 12:39:19,482 - INFO - Mean valid loss: 0.0247 score: 0.02453\n2022-10-04 12:39:19,489 - INFO - Starting 13 epoch...\n2022-10-04 12:39:21,004 - INFO - Mean train loss: 0.02591\n2022-10-04 12:39:23,336 - INFO - valid loss: 0.02465 valid loss aug: 0.02469\n2022-10-04 12:39:23,340 - INFO - Mean valid loss: 0.02467 score: 0.02463\n2022-10-04 12:39:23,343 - INFO - Starting 14 epoch...\n2022-10-04 12:39:24,784 - INFO - Mean train loss: 0.02488\n2022-10-04 12:39:27,085 - INFO - valid loss: 0.02459 valid loss aug: 0.02479\n2022-10-04 12:39:27,088 - INFO - Mean valid loss: 0.02469 score: 0.02454\n2022-10-04 12:39:27,091 - INFO - Starting 15 epoch...\n2022-10-04 12:39:28,535 - INFO - Mean train loss: 0.02418\n2022-10-04 12:39:30,859 - INFO - valid loss: 0.02419 valid loss aug: 0.02471\n2022-10-04 12:39:30,862 - INFO - Mean valid loss: 0.02445 score: 0.02451\n2022-10-04 12:39:30,870 - INFO - Starting 16 epoch...\n2022-10-04 12:39:32,336 - INFO - Mean train loss: 0.0241\n2022-10-04 12:39:34,667 - INFO - valid loss: 0.02438 valid loss aug: 0.02484\n2022-10-04 12:39:34,670 - INFO - Mean valid loss: 0.02461 score: 0.02462\n2022-10-04 12:39:34,673 - INFO - Starting 17 epoch...\n2022-10-04 12:39:36,119 - INFO - Mean train loss: 0.02421\n2022-10-04 12:39:38,438 - INFO - valid loss: 0.02458 valid loss aug: 0.02485\n2022-10-04 12:39:38,441 - INFO - Mean valid loss: 0.02471 score: 0.0247\n2022-10-04 12:39:38,443 - INFO - Starting 18 epoch...\n2022-10-04 12:39:39,890 - INFO - Mean train loss: 0.02424\n2022-10-04 12:39:42,208 - INFO - valid loss: 0.02429 valid loss aug: 0.02445\n2022-10-04 12:39:42,211 - INFO - Mean valid loss: 0.02437 score: 0.02432\n2022-10-04 12:39:42,217 - INFO - Starting 19 epoch...\n2022-10-04 12:39:43,681 - INFO - Mean train loss: 0.02397\n2022-10-04 12:39:46,087 - INFO - valid loss: 0.02486 valid loss aug: 0.02507\n2022-10-04 12:39:46,090 - INFO - Mean valid loss: 0.02497 score: 0.02493\n2022-10-04 12:39:46,093 - INFO - Starting 20 epoch...\n2022-10-04 12:39:47,539 - INFO - Mean train loss: 0.02557\n2022-10-04 12:39:49,864 - INFO - valid loss: 0.02443 valid loss aug: 0.02484\n2022-10-04 12:39:49,868 - INFO - Mean valid loss: 0.02463 score: 0.02456\n2022-10-04 12:39:49,870 - INFO - Starting 21 epoch...\n2022-10-04 12:39:51,319 - INFO - Mean train loss: 0.02446\n2022-10-04 12:39:53,737 - INFO - valid loss: 0.02452 valid loss aug: 0.02501\n2022-10-04 12:39:53,741 - INFO - Mean valid loss: 0.02476 score: 0.02462\n2022-10-04 12:39:53,744 - INFO - Starting 22 epoch...\n2022-10-04 12:39:55,459 - INFO - Mean train loss: 0.02439\n2022-10-04 12:39:57,955 - INFO - valid loss: 0.02438 valid loss aug: 0.02482\n2022-10-04 12:39:57,960 - INFO - Mean valid loss: 0.0246 score: 0.02452\n2022-10-04 12:39:57,964 - INFO - Starting 23 epoch...\n2022-10-04 12:40:00,650 - INFO - Mean train loss: 0.02374\n2022-10-04 12:40:03,758 - INFO - valid loss: 0.0244 valid loss aug: 0.02481\n2022-10-04 12:40:03,761 - INFO - Mean valid loss: 0.02461 score: 0.02458\n2022-10-04 12:40:03,764 - INFO - Starting 24 epoch...\n2022-10-04 12:40:05,278 - INFO - Mean train loss: 0.02408\n2022-10-04 12:40:07,587 - INFO - valid loss: 0.02439 valid loss aug: 0.02474\n2022-10-04 12:40:07,590 - INFO - Mean valid loss: 0.02457 score: 0.02453\n2022-10-04 12:40:07,592 - INFO - Starting 25 epoch...\n2022-10-04 12:40:09,034 - INFO - Mean train loss: 0.02428\n2022-10-04 12:40:11,350 - INFO - valid loss: 0.0242 valid loss aug: 0.02474\n2022-10-04 12:40:11,352 - INFO - Mean valid loss: 0.02447 score: 0.0243\n2022-10-04 12:40:11,360 - INFO - Starting 26 epoch...\n2022-10-04 12:40:12,805 - INFO - Mean train loss: 0.02354\n2022-10-04 12:40:15,138 - INFO - valid loss: 0.0244 valid loss aug: 0.02476\n2022-10-04 12:40:15,141 - INFO - Mean valid loss: 0.02458 score: 0.02442\n2022-10-04 12:40:15,143 - INFO - Starting 27 epoch...\n2022-10-04 12:40:16,746 - INFO - Mean train loss: 0.02376\n2022-10-04 12:40:19,181 - INFO - valid loss: 0.02439 valid loss aug: 0.02476\n2022-10-04 12:40:19,184 - INFO - Mean valid loss: 0.02457 score: 0.02452\n2022-10-04 12:40:19,186 - INFO - Starting 28 epoch...\n2022-10-04 12:40:20,648 - INFO - Mean train loss: 0.02406\n2022-10-04 12:40:22,999 - INFO - valid loss: 0.02401 valid loss aug: 0.02465\n2022-10-04 12:40:23,002 - INFO - Mean valid loss: 0.02433 score: 0.02427\n2022-10-04 12:40:23,010 - INFO - Starting 29 epoch...\n2022-10-04 12:40:24,467 - INFO - Mean train loss: 0.02403\n2022-10-04 12:40:26,811 - INFO - valid loss: 0.02432 valid loss aug: 0.0247\n2022-10-04 12:40:26,815 - INFO - Mean valid loss: 0.02451 score: 0.0243\n2022-10-04 12:40:26,817 - INFO - Starting 30 epoch...\n2022-10-04 12:40:28,431 - INFO - Mean train loss: 0.02374\n2022-10-04 12:40:30,782 - INFO - valid loss: 0.0243 valid loss aug: 0.02481\n2022-10-04 12:40:30,785 - INFO - Mean valid loss: 0.02456 score: 0.02448\n2022-10-04 12:40:30,787 - INFO - Starting 31 epoch...\n2022-10-04 12:40:32,250 - INFO - Mean train loss: 0.02484\n2022-10-04 12:40:34,609 - INFO - valid loss: 0.02418 valid loss aug: 0.02474\n2022-10-04 12:40:34,614 - INFO - Mean valid loss: 0.02446 score: 0.02423\n2022-10-04 12:40:34,622 - INFO - Starting 32 epoch...\n2022-10-04 12:40:36,067 - INFO - Mean train loss: 0.02422\n2022-10-04 12:40:38,417 - INFO - valid loss: 0.02446 valid loss aug: 0.02508\n2022-10-04 12:40:38,419 - INFO - Mean valid loss: 0.02477 score: 0.0246\n2022-10-04 12:40:38,421 - INFO - Starting 33 epoch...\n2022-10-04 12:40:39,882 - INFO - Mean train loss: 0.02364\n2022-10-04 12:40:42,203 - INFO - valid loss: 0.02445 valid loss aug: 0.02486\n2022-10-04 12:40:42,206 - INFO - Mean valid loss: 0.02465 score: 0.0246\n2022-10-04 12:40:42,210 - INFO - Starting 34 epoch...\n2022-10-04 12:40:43,729 - INFO - Mean train loss: 0.02353\n2022-10-04 12:40:46,083 - INFO - valid loss: 0.02422 valid loss aug: 0.02488\n2022-10-04 12:40:46,086 - INFO - Mean valid loss: 0.02455 score: 0.0245\n2022-10-04 12:40:46,089 - INFO - Starting 35 epoch...\n2022-10-04 12:40:47,513 - INFO - Mean train loss: 0.02488\n2022-10-04 12:40:49,931 - INFO - valid loss: 0.02426 valid loss aug: 0.02468\n2022-10-04 12:40:49,934 - INFO - Mean valid loss: 0.02447 score: 0.02437\n2022-10-04 12:40:49,936 - INFO - Starting 36 epoch...\n2022-10-04 12:40:51,384 - INFO - Mean train loss: 0.02392\n2022-10-04 12:40:53,748 - INFO - valid loss: 0.02454 valid loss aug: 0.02495\n2022-10-04 12:40:53,751 - INFO - Mean valid loss: 0.02474 score: 0.02457\n2022-10-04 12:40:53,753 - INFO - Starting 37 epoch...\n2022-10-04 12:40:55,205 - INFO - Mean train loss: 0.02356\n2022-10-04 12:40:57,547 - INFO - valid loss: 0.02427 valid loss aug: 0.02478\n2022-10-04 12:40:57,550 - INFO - Mean valid loss: 0.02452 score: 0.02429\n2022-10-04 12:40:57,552 - INFO - Starting 38 epoch...\n2022-10-04 12:40:59,011 - INFO - Mean train loss: 0.02337\n2022-10-04 12:41:01,343 - INFO - valid loss: 0.02424 valid loss aug: 0.02485\n2022-10-04 12:41:01,346 - INFO - Mean valid loss: 0.02454 score: 0.02426\n2022-10-04 12:41:01,348 - INFO - Starting 39 epoch...\n2022-10-04 12:41:02,790 - INFO - Mean train loss: 0.02365\n2022-10-04 12:41:05,113 - INFO - valid loss: 0.02465 valid loss aug: 0.02493\n2022-10-04 12:41:05,116 - INFO - Mean valid loss: 0.02479 score: 0.02444\n2022-10-04 12:41:05,118 - INFO - Starting 40 epoch...\n2022-10-04 12:41:06,540 - INFO - Mean train loss: 0.02463\n2022-10-04 12:41:08,858 - INFO - valid loss: 0.02415 valid loss aug: 0.02507\n2022-10-04 12:41:08,863 - INFO - Mean valid loss: 0.02461 score: 0.02425\n2022-10-04 12:41:08,865 - INFO - Starting 41 epoch...\n2022-10-04 12:41:10,327 - INFO - Mean train loss: 0.02387\n2022-10-04 12:41:12,658 - INFO - valid loss: 0.02429 valid loss aug: 0.02499\n2022-10-04 12:41:12,661 - INFO - Mean valid loss: 0.02464 score: 0.02438\n2022-10-04 12:41:12,663 - INFO - Starting 42 epoch...\n2022-10-04 12:41:14,098 - INFO - Mean train loss: 0.02323\n2022-10-04 12:41:16,396 - INFO - valid loss: 0.02429 valid loss aug: 0.025\n2022-10-04 12:41:16,399 - INFO - Mean valid loss: 0.02464 score: 0.02444\n2022-10-04 12:41:16,402 - INFO - Starting 43 epoch...\n2022-10-04 12:41:17,824 - INFO - Mean train loss: 0.02343\n2022-10-04 12:41:20,194 - INFO - valid loss: 0.02424 valid loss aug: 0.02486\n2022-10-04 12:41:20,197 - INFO - Mean valid loss: 0.02455 score: 0.02436\n2022-10-04 12:41:20,199 - INFO - Starting 44 epoch...\n2022-10-04 12:41:21,635 - INFO - Mean train loss: 0.02356\n2022-10-04 12:41:23,972 - INFO - valid loss: 0.02453 valid loss aug: 0.02509\n2022-10-04 12:41:23,976 - INFO - Mean valid loss: 0.02481 score: 0.02448\n2022-10-04 12:41:23,978 - INFO - Starting 45 epoch...\n2022-10-04 12:41:25,394 - INFO - Mean train loss: 0.02421\n2022-10-04 12:41:27,683 - INFO - valid loss: 0.02456 valid loss aug: 0.02511\n2022-10-04 12:41:27,686 - INFO - Mean valid loss: 0.02484 score: 0.02458\n2022-10-04 12:41:27,688 - INFO - Starting 46 epoch...\n2022-10-04 12:41:29,114 - INFO - Mean train loss: 0.02297\n2022-10-04 12:41:31,389 - INFO - valid loss: 0.02505 valid loss aug: 0.02513\n2022-10-04 12:41:31,392 - INFO - Mean valid loss: 0.02509 score: 0.02486\n2022-10-04 12:41:31,395 - INFO - Starting 47 epoch...\n2022-10-04 12:41:32,806 - INFO - Mean train loss: 0.02275\n2022-10-04 12:41:35,136 - INFO - valid loss: 0.02417 valid loss aug: 0.02473\n2022-10-04 12:41:35,139 - INFO - Mean valid loss: 0.02445 score: 0.02424\n2022-10-04 12:41:35,142 - INFO - Starting 48 epoch...\n2022-10-04 12:41:36,572 - INFO - Mean train loss: 0.02404\n2022-10-04 12:41:38,864 - INFO - valid loss: 0.02418 valid loss aug: 0.02486\n2022-10-04 12:41:38,867 - INFO - Mean valid loss: 0.02452 score: 0.02423\n2022-10-04 12:41:38,876 - INFO - Starting 49 epoch...\n2022-10-04 12:41:40,316 - INFO - Mean train loss: 0.02269\n2022-10-04 12:41:42,607 - INFO - valid loss: 0.02497 valid loss aug: 0.02541\n2022-10-04 12:41:42,610 - INFO - Mean valid loss: 0.02519 score: 0.02473\n2022-10-04 12:41:42,614 - INFO - Starting 50 epoch...\n2022-10-04 12:41:44,076 - INFO - Mean train loss: 0.02301\n2022-10-04 12:41:46,380 - INFO - valid loss: 0.02407 valid loss aug: 0.02484\n2022-10-04 12:41:46,383 - INFO - Mean valid loss: 0.02446 score: 0.02411\n2022-10-04 12:41:46,390 - INFO - best score=0.024114 on epoch=50 fold=2\n2022-10-04 12:41:46,391 - INFO - [train fold2] done in 192.32 s\n2022-10-04 12:41:46,547 - INFO - [create model] done in 0.15 s\n2022-10-04 12:41:46,550 - INFO - Starting 1 epoch...\n2022-10-04 12:41:48,010 - INFO - Mean train loss: 0.04086\n2022-10-04 12:41:50,372 - INFO - valid loss: 0.02327 valid loss aug: 0.02337\n2022-10-04 12:41:50,375 - INFO - Mean valid loss: 0.02332 score: 0.02313\n2022-10-04 12:41:50,382 - INFO - Starting 2 epoch...\n2022-10-04 12:41:52,084 - INFO - Mean train loss: 0.02599\n2022-10-04 12:41:54,478 - INFO - valid loss: 0.02293 valid loss aug: 0.02295\n2022-10-04 12:41:54,481 - INFO - Mean valid loss: 0.02294 score: 0.02287\n2022-10-04 12:41:54,488 - INFO - Starting 3 epoch...\n2022-10-04 12:41:56,001 - INFO - Mean train loss: 0.02577\n2022-10-04 12:41:58,341 - INFO - valid loss: 0.02283 valid loss aug: 0.02281\n2022-10-04 12:41:58,343 - INFO - Mean valid loss: 0.02282 score: 0.0227\n2022-10-04 12:41:58,351 - INFO - Starting 4 epoch...\n2022-10-04 12:41:59,823 - INFO - Mean train loss: 0.02539\n2022-10-04 12:42:02,142 - INFO - valid loss: 0.02259 valid loss aug: 0.02277\n2022-10-04 12:42:02,145 - INFO - Mean valid loss: 0.02268 score: 0.02262\n2022-10-04 12:42:02,152 - INFO - Starting 5 epoch...\n2022-10-04 12:42:03,629 - INFO - Mean train loss: 0.02513\n2022-10-04 12:42:05,982 - INFO - valid loss: 0.0229 valid loss aug: 0.0229\n2022-10-04 12:42:05,985 - INFO - Mean valid loss: 0.0229 score: 0.02271\n2022-10-04 12:42:05,988 - INFO - Starting 6 epoch...\n2022-10-04 12:42:07,424 - INFO - Mean train loss: 0.02545\n2022-10-04 12:42:09,735 - INFO - valid loss: 0.02279 valid loss aug: 0.0228\n2022-10-04 12:42:09,737 - INFO - Mean valid loss: 0.0228 score: 0.02269\n2022-10-04 12:42:09,740 - INFO - Starting 7 epoch...\n2022-10-04 12:42:11,191 - INFO - Mean train loss: 0.02533\n2022-10-04 12:42:13,500 - INFO - valid loss: 0.02264 valid loss aug: 0.02266\n2022-10-04 12:42:13,503 - INFO - Mean valid loss: 0.02265 score: 0.02259\n2022-10-04 12:42:13,510 - INFO - Starting 8 epoch...\n2022-10-04 12:42:14,950 - INFO - Mean train loss: 0.02535\n2022-10-04 12:42:17,272 - INFO - valid loss: 0.0229 valid loss aug: 0.0232\n2022-10-04 12:42:17,275 - INFO - Mean valid loss: 0.02305 score: 0.02262\n2022-10-04 12:42:17,277 - INFO - Starting 9 epoch...\n2022-10-04 12:42:18,822 - INFO - Mean train loss: 0.02558\n2022-10-04 12:42:21,228 - INFO - valid loss: 0.02257 valid loss aug: 0.02297\n2022-10-04 12:42:21,233 - INFO - Mean valid loss: 0.02277 score: 0.02259\n2022-10-04 12:42:21,235 - INFO - Starting 10 epoch...\n2022-10-04 12:42:23,057 - INFO - Mean train loss: 0.02666\n2022-10-04 12:42:25,384 - INFO - valid loss: 0.02252 valid loss aug: 0.02282\n2022-10-04 12:42:25,386 - INFO - Mean valid loss: 0.02267 score: 0.02252\n2022-10-04 12:42:25,393 - INFO - Starting 11 epoch...\n2022-10-04 12:42:26,876 - INFO - Mean train loss: 0.02515\n2022-10-04 12:42:29,218 - INFO - valid loss: 0.02277 valid loss aug: 0.02266\n2022-10-04 12:42:29,221 - INFO - Mean valid loss: 0.02271 score: 0.02261\n2022-10-04 12:42:29,223 - INFO - Starting 12 epoch...\n2022-10-04 12:42:30,678 - INFO - Mean train loss: 0.02483\n2022-10-04 12:42:32,992 - INFO - valid loss: 0.02243 valid loss aug: 0.02268\n2022-10-04 12:42:32,995 - INFO - Mean valid loss: 0.02255 score: 0.02247\n2022-10-04 12:42:33,002 - INFO - Starting 13 epoch...\n2022-10-04 12:42:34,449 - INFO - Mean train loss: 0.02473\n2022-10-04 12:42:36,771 - INFO - valid loss: 0.02272 valid loss aug: 0.0232\n2022-10-04 12:42:36,774 - INFO - Mean valid loss: 0.02296 score: 0.02266\n2022-10-04 12:42:36,777 - INFO - Starting 14 epoch...\n2022-10-04 12:42:38,233 - INFO - Mean train loss: 0.02493\n2022-10-04 12:42:40,581 - INFO - valid loss: 0.02255 valid loss aug: 0.02267\n2022-10-04 12:42:40,584 - INFO - Mean valid loss: 0.02261 score: 0.02248\n2022-10-04 12:42:40,586 - INFO - Starting 15 epoch...\n2022-10-04 12:42:42,038 - INFO - Mean train loss: 0.02491\n2022-10-04 12:42:44,352 - INFO - valid loss: 0.02308 valid loss aug: 0.02329\n2022-10-04 12:42:44,355 - INFO - Mean valid loss: 0.02318 score: 0.02295\n2022-10-04 12:42:44,358 - INFO - Starting 16 epoch...\n2022-10-04 12:42:45,815 - INFO - Mean train loss: 0.02485\n2022-10-04 12:42:48,117 - INFO - valid loss: 0.02247 valid loss aug: 0.02259\n2022-10-04 12:42:48,120 - INFO - Mean valid loss: 0.02253 score: 0.0224\n2022-10-04 12:42:48,128 - INFO - Starting 17 epoch...\n2022-10-04 12:42:49,582 - INFO - Mean train loss: 0.02516\n2022-10-04 12:42:51,916 - INFO - valid loss: 0.02265 valid loss aug: 0.02287\n2022-10-04 12:42:51,920 - INFO - Mean valid loss: 0.02276 score: 0.02247\n2022-10-04 12:42:51,922 - INFO - Starting 18 epoch...\n2022-10-04 12:42:53,556 - INFO - Mean train loss: 0.02498\n2022-10-04 12:42:55,923 - INFO - valid loss: 0.02252 valid loss aug: 0.0229\n2022-10-04 12:42:55,926 - INFO - Mean valid loss: 0.02271 score: 0.02251\n2022-10-04 12:42:55,928 - INFO - Starting 19 epoch...\n2022-10-04 12:42:57,416 - INFO - Mean train loss: 0.02493\n2022-10-04 12:42:59,752 - INFO - valid loss: 0.02263 valid loss aug: 0.02274\n2022-10-04 12:42:59,754 - INFO - Mean valid loss: 0.02268 score: 0.02252\n2022-10-04 12:42:59,756 - INFO - Starting 20 epoch...\n2022-10-04 12:43:01,240 - INFO - Mean train loss: 0.02504\n2022-10-04 12:43:03,617 - INFO - valid loss: 0.0226 valid loss aug: 0.02274\n2022-10-04 12:43:03,620 - INFO - Mean valid loss: 0.02267 score: 0.02247\n2022-10-04 12:43:03,622 - INFO - Starting 21 epoch...\n2022-10-04 12:43:05,120 - INFO - Mean train loss: 0.02468\n2022-10-04 12:43:07,475 - INFO - valid loss: 0.02263 valid loss aug: 0.02287\n2022-10-04 12:43:07,478 - INFO - Mean valid loss: 0.02275 score: 0.02249\n2022-10-04 12:43:07,480 - INFO - Starting 22 epoch...\n2022-10-04 12:43:08,964 - INFO - Mean train loss: 0.0253\n2022-10-04 12:43:11,311 - INFO - valid loss: 0.02277 valid loss aug: 0.02312\n2022-10-04 12:43:11,315 - INFO - Mean valid loss: 0.02295 score: 0.02272\n2022-10-04 12:43:11,317 - INFO - Starting 23 epoch...\n2022-10-04 12:43:12,775 - INFO - Mean train loss: 0.02466\n2022-10-04 12:43:15,100 - INFO - valid loss: 0.0227 valid loss aug: 0.02288\n2022-10-04 12:43:15,103 - INFO - Mean valid loss: 0.02279 score: 0.02257\n2022-10-04 12:43:15,106 - INFO - Starting 24 epoch...\n2022-10-04 12:43:16,543 - INFO - Mean train loss: 0.02478\n2022-10-04 12:43:18,842 - INFO - valid loss: 0.02261 valid loss aug: 0.02278\n2022-10-04 12:43:18,845 - INFO - Mean valid loss: 0.0227 score: 0.02248\n2022-10-04 12:43:18,847 - INFO - Starting 25 epoch...\n2022-10-04 12:43:20,297 - INFO - Mean train loss: 0.02466\n2022-10-04 12:43:22,593 - INFO - valid loss: 0.02257 valid loss aug: 0.02281\n2022-10-04 12:43:22,596 - INFO - Mean valid loss: 0.02269 score: 0.02249\n2022-10-04 12:43:22,599 - INFO - Starting 26 epoch...\n2022-10-04 12:43:24,096 - INFO - Mean train loss: 0.02435\n2022-10-04 12:43:26,488 - INFO - valid loss: 0.02285 valid loss aug: 0.02284\n2022-10-04 12:43:26,490 - INFO - Mean valid loss: 0.02284 score: 0.02252\n2022-10-04 12:43:26,493 - INFO - Starting 27 epoch...\n2022-10-04 12:43:27,969 - INFO - Mean train loss: 0.02448\n2022-10-04 12:43:30,308 - INFO - valid loss: 0.02238 valid loss aug: 0.02259\n2022-10-04 12:43:30,311 - INFO - Mean valid loss: 0.02249 score: 0.02225\n2022-10-04 12:43:30,318 - INFO - Starting 28 epoch...\n2022-10-04 12:43:31,784 - INFO - Mean train loss: 0.02428\n2022-10-04 12:43:34,116 - INFO - valid loss: 0.02284 valid loss aug: 0.02327\n2022-10-04 12:43:34,119 - INFO - Mean valid loss: 0.02305 score: 0.02274\n2022-10-04 12:43:34,122 - INFO - Starting 29 epoch...\n2022-10-04 12:43:35,561 - INFO - Mean train loss: 0.02434\n2022-10-04 12:43:37,853 - INFO - valid loss: 0.02263 valid loss aug: 0.02296\n2022-10-04 12:43:37,856 - INFO - Mean valid loss: 0.02279 score: 0.02249\n2022-10-04 12:43:37,859 - INFO - Starting 30 epoch...\n2022-10-04 12:43:39,292 - INFO - Mean train loss: 0.02595\n2022-10-04 12:43:41,605 - INFO - valid loss: 0.02296 valid loss aug: 0.02311\n2022-10-04 12:43:41,608 - INFO - Mean valid loss: 0.02303 score: 0.02271\n2022-10-04 12:43:41,610 - INFO - Starting 31 epoch...\n2022-10-04 12:43:43,049 - INFO - Mean train loss: 0.02437\n2022-10-04 12:43:45,341 - INFO - valid loss: 0.02253 valid loss aug: 0.02317\n2022-10-04 12:43:45,344 - INFO - Mean valid loss: 0.02285 score: 0.02228\n2022-10-04 12:43:45,346 - INFO - Starting 32 epoch...\n2022-10-04 12:43:46,783 - INFO - Mean train loss: 0.02427\n2022-10-04 12:43:49,070 - INFO - valid loss: 0.0226 valid loss aug: 0.02295\n2022-10-04 12:43:49,073 - INFO - Mean valid loss: 0.02277 score: 0.02239\n2022-10-04 12:43:49,075 - INFO - Starting 33 epoch...\n2022-10-04 12:43:50,521 - INFO - Mean train loss: 0.02447\n2022-10-04 12:43:52,829 - INFO - valid loss: 0.02279 valid loss aug: 0.02306\n2022-10-04 12:43:52,832 - INFO - Mean valid loss: 0.02292 score: 0.02256\n2022-10-04 12:43:52,835 - INFO - Starting 34 epoch...\n2022-10-04 12:43:54,272 - INFO - Mean train loss: 0.02405\n2022-10-04 12:43:56,683 - INFO - valid loss: 0.02274 valid loss aug: 0.02287\n2022-10-04 12:43:56,686 - INFO - Mean valid loss: 0.02281 score: 0.02244\n2022-10-04 12:43:56,688 - INFO - Starting 35 epoch...\n2022-10-04 12:43:58,173 - INFO - Mean train loss: 0.024\n2022-10-04 12:44:00,540 - INFO - valid loss: 0.02288 valid loss aug: 0.02288\n2022-10-04 12:44:00,543 - INFO - Mean valid loss: 0.02288 score: 0.02251\n2022-10-04 12:44:00,545 - INFO - Starting 36 epoch...\n2022-10-04 12:44:02,024 - INFO - Mean train loss: 0.02395\n2022-10-04 12:44:04,340 - INFO - valid loss: 0.02268 valid loss aug: 0.02287\n2022-10-04 12:44:04,343 - INFO - Mean valid loss: 0.02278 score: 0.02243\n2022-10-04 12:44:04,346 - INFO - Starting 37 epoch...\n2022-10-04 12:44:05,795 - INFO - Mean train loss: 0.02381\n2022-10-04 12:44:08,113 - INFO - valid loss: 0.02281 valid loss aug: 0.02344\n2022-10-04 12:44:08,116 - INFO - Mean valid loss: 0.02313 score: 0.02264\n2022-10-04 12:44:08,119 - INFO - Starting 38 epoch...\n2022-10-04 12:44:09,593 - INFO - Mean train loss: 0.02424\n2022-10-04 12:44:11,918 - INFO - valid loss: 0.02242 valid loss aug: 0.02283\n2022-10-04 12:44:11,921 - INFO - Mean valid loss: 0.02262 score: 0.02239\n2022-10-04 12:44:11,923 - INFO - Starting 39 epoch...\n2022-10-04 12:44:13,351 - INFO - Mean train loss: 0.02366\n2022-10-04 12:44:15,647 - INFO - valid loss: 0.02292 valid loss aug: 0.02337\n2022-10-04 12:44:15,650 - INFO - Mean valid loss: 0.02314 score: 0.02273\n2022-10-04 12:44:15,652 - INFO - Starting 40 epoch...\n2022-10-04 12:44:17,101 - INFO - Mean train loss: 0.02416\n2022-10-04 12:44:19,423 - INFO - valid loss: 0.02289 valid loss aug: 0.0237\n2022-10-04 12:44:19,426 - INFO - Mean valid loss: 0.02329 score: 0.02286\n2022-10-04 12:44:19,428 - INFO - Starting 41 epoch...\n2022-10-04 12:44:20,870 - INFO - Mean train loss: 0.02442\n2022-10-04 12:44:23,191 - INFO - valid loss: 0.02289 valid loss aug: 0.02324\n2022-10-04 12:44:23,194 - INFO - Mean valid loss: 0.02306 score: 0.0226\n2022-10-04 12:44:23,196 - INFO - Starting 42 epoch...\n2022-10-04 12:44:24,651 - INFO - Mean train loss: 0.02472\n2022-10-04 12:44:27,018 - INFO - valid loss: 0.02285 valid loss aug: 0.02335\n2022-10-04 12:44:27,021 - INFO - Mean valid loss: 0.0231 score: 0.02267\n2022-10-04 12:44:27,024 - INFO - Starting 43 epoch...\n2022-10-04 12:44:28,690 - INFO - Mean train loss: 0.02342\n2022-10-04 12:44:31,058 - INFO - valid loss: 0.02277 valid loss aug: 0.02304\n2022-10-04 12:44:31,061 - INFO - Mean valid loss: 0.0229 score: 0.02261\n2022-10-04 12:44:31,063 - INFO - Starting 44 epoch...\n2022-10-04 12:44:32,547 - INFO - Mean train loss: 0.0245\n2022-10-04 12:44:34,897 - INFO - valid loss: 0.02301 valid loss aug: 0.0237\n2022-10-04 12:44:34,900 - INFO - Mean valid loss: 0.02336 score: 0.02285\n2022-10-04 12:44:34,902 - INFO - Starting 45 epoch...\n2022-10-04 12:44:36,341 - INFO - Mean train loss: 0.02465\n2022-10-04 12:44:38,643 - INFO - valid loss: 0.02284 valid loss aug: 0.02335\n2022-10-04 12:44:38,646 - INFO - Mean valid loss: 0.02309 score: 0.02261\n2022-10-04 12:44:38,648 - INFO - Starting 46 epoch...\n2022-10-04 12:44:40,136 - INFO - Mean train loss: 0.02353\n2022-10-04 12:44:42,463 - INFO - valid loss: 0.02286 valid loss aug: 0.0232\n2022-10-04 12:44:42,466 - INFO - Mean valid loss: 0.02303 score: 0.0227\n2022-10-04 12:44:42,469 - INFO - Starting 47 epoch...\n2022-10-04 12:44:43,925 - INFO - Mean train loss: 0.02317\n2022-10-04 12:44:46,270 - INFO - valid loss: 0.02289 valid loss aug: 0.0235\n2022-10-04 12:44:46,272 - INFO - Mean valid loss: 0.0232 score: 0.02267\n2022-10-04 12:44:46,275 - INFO - Starting 48 epoch...\n2022-10-04 12:44:47,705 - INFO - Mean train loss: 0.02497\n2022-10-04 12:44:50,035 - INFO - valid loss: 0.02297 valid loss aug: 0.02332\n2022-10-04 12:44:50,039 - INFO - Mean valid loss: 0.02315 score: 0.02257\n2022-10-04 12:44:50,041 - INFO - Starting 49 epoch...\n2022-10-04 12:44:51,545 - INFO - Mean train loss: 0.02338\n2022-10-04 12:44:53,856 - INFO - valid loss: 0.02301 valid loss aug: 0.02338\n2022-10-04 12:44:53,859 - INFO - Mean valid loss: 0.02319 score: 0.02261\n2022-10-04 12:44:53,862 - INFO - Starting 50 epoch...\n2022-10-04 12:44:55,298 - INFO - Mean train loss: 0.02281\n2022-10-04 12:44:57,608 - INFO - valid loss: 0.02316 valid loss aug: 0.02336\n2022-10-04 12:44:57,611 - INFO - Mean valid loss: 0.02326 score: 0.02281\n2022-10-04 12:44:57,612 - INFO - best score=0.02225 on epoch=27 fold=3\n2022-10-04 12:44:57,613 - INFO - [train fold3] done in 191.06 s\n2022-10-04 12:44:57,774 - INFO - [create model] done in 0.16 s\n2022-10-04 12:44:57,777 - INFO - Starting 1 epoch...\n2022-10-04 12:44:59,564 - INFO - Mean train loss: 0.0401\n2022-10-04 12:45:01,889 - INFO - valid loss: 0.02696 valid loss aug: 0.02695\n2022-10-04 12:45:01,894 - INFO - Mean valid loss: 0.02696 score: 0.02671\n2022-10-04 12:45:01,903 - INFO - Starting 2 epoch...\n2022-10-04 12:45:03,336 - INFO - Mean train loss: 0.02656\n2022-10-04 12:45:05,666 - INFO - valid loss: 0.02659 valid loss aug: 0.02661\n2022-10-04 12:45:05,669 - INFO - Mean valid loss: 0.0266 score: 0.02639\n2022-10-04 12:45:05,677 - INFO - Starting 3 epoch...\n2022-10-04 12:45:07,098 - INFO - Mean train loss: 0.02531\n2022-10-04 12:45:09,403 - INFO - valid loss: 0.02626 valid loss aug: 0.02624\n2022-10-04 12:45:09,406 - INFO - Mean valid loss: 0.02625 score: 0.02618\n2022-10-04 12:45:09,414 - INFO - Starting 4 epoch...\n2022-10-04 12:45:10,870 - INFO - Mean train loss: 0.02617\n2022-10-04 12:45:13,330 - INFO - valid loss: 0.02602 valid loss aug: 0.02609\n2022-10-04 12:45:13,336 - INFO - Mean valid loss: 0.02605 score: 0.02604\n2022-10-04 12:45:13,355 - INFO - Starting 5 epoch...\n2022-10-04 12:45:14,898 - INFO - Mean train loss: 0.02455\n2022-10-04 12:45:17,215 - INFO - valid loss: 0.02628 valid loss aug: 0.02599\n2022-10-04 12:45:17,218 - INFO - Mean valid loss: 0.02614 score: 0.02603\n2022-10-04 12:45:17,225 - INFO - Starting 6 epoch...\n2022-10-04 12:45:18,680 - INFO - Mean train loss: 0.02452\n2022-10-04 12:45:20,979 - INFO - valid loss: 0.02592 valid loss aug: 0.02594\n2022-10-04 12:45:20,981 - INFO - Mean valid loss: 0.02593 score: 0.02581\n2022-10-04 12:45:20,990 - INFO - Starting 7 epoch...\n2022-10-04 12:45:22,413 - INFO - Mean train loss: 0.02425\n2022-10-04 12:45:24,728 - INFO - valid loss: 0.02592 valid loss aug: 0.02579\n2022-10-04 12:45:24,732 - INFO - Mean valid loss: 0.02586 score: 0.02578\n2022-10-04 12:45:24,740 - INFO - Starting 8 epoch...\n2022-10-04 12:45:26,178 - INFO - Mean train loss: 0.02528\n2022-10-04 12:45:28,467 - INFO - valid loss: 0.02573 valid loss aug: 0.02572\n2022-10-04 12:45:28,470 - INFO - Mean valid loss: 0.02572 score: 0.02573\n2022-10-04 12:45:28,477 - INFO - Starting 9 epoch...\n2022-10-04 12:45:30,166 - INFO - Mean train loss: 0.02444\n2022-10-04 12:45:32,499 - INFO - valid loss: 0.02598 valid loss aug: 0.0257\n2022-10-04 12:45:32,501 - INFO - Mean valid loss: 0.02584 score: 0.0258\n2022-10-04 12:45:32,504 - INFO - Starting 10 epoch...\n2022-10-04 12:45:33,942 - INFO - Mean train loss: 0.0247\n2022-10-04 12:45:36,272 - INFO - valid loss: 0.02587 valid loss aug: 0.02581\n2022-10-04 12:45:36,274 - INFO - Mean valid loss: 0.02584 score: 0.0258\n2022-10-04 12:45:36,276 - INFO - Starting 11 epoch...\n2022-10-04 12:45:37,734 - INFO - Mean train loss: 0.0245\n2022-10-04 12:45:40,032 - INFO - valid loss: 0.02554 valid loss aug: 0.02562\n2022-10-04 12:45:40,035 - INFO - Mean valid loss: 0.02558 score: 0.02551\n2022-10-04 12:45:40,043 - INFO - Starting 12 epoch...\n2022-10-04 12:45:41,489 - INFO - Mean train loss: 0.02423\n2022-10-04 12:45:43,785 - INFO - valid loss: 0.02569 valid loss aug: 0.02576\n2022-10-04 12:45:43,788 - INFO - Mean valid loss: 0.02572 score: 0.02568\n2022-10-04 12:45:43,791 - INFO - Starting 13 epoch...\n2022-10-04 12:45:45,230 - INFO - Mean train loss: 0.02409\n2022-10-04 12:45:47,533 - INFO - valid loss: 0.02544 valid loss aug: 0.0256\n2022-10-04 12:45:47,536 - INFO - Mean valid loss: 0.02552 score: 0.02545\n2022-10-04 12:45:47,543 - INFO - Starting 14 epoch...\n2022-10-04 12:45:48,952 - INFO - Mean train loss: 0.02512\n2022-10-04 12:45:51,251 - INFO - valid loss: 0.02558 valid loss aug: 0.02553\n2022-10-04 12:45:51,254 - INFO - Mean valid loss: 0.02556 score: 0.02546\n2022-10-04 12:45:51,256 - INFO - Starting 15 epoch...\n2022-10-04 12:45:52,679 - INFO - Mean train loss: 0.02402\n2022-10-04 12:45:54,971 - INFO - valid loss: 0.02561 valid loss aug: 0.0255\n2022-10-04 12:45:54,973 - INFO - Mean valid loss: 0.02555 score: 0.02532\n2022-10-04 12:45:54,981 - INFO - Starting 16 epoch...\n2022-10-04 12:45:56,426 - INFO - Mean train loss: 0.02411\n2022-10-04 12:45:58,777 - INFO - valid loss: 0.02561 valid loss aug: 0.02543\n2022-10-04 12:45:58,780 - INFO - Mean valid loss: 0.02552 score: 0.02543\n2022-10-04 12:45:58,783 - INFO - Starting 17 epoch...\n2022-10-04 12:46:00,234 - INFO - Mean train loss: 0.02395\n2022-10-04 12:46:02,637 - INFO - valid loss: 0.02549 valid loss aug: 0.02563\n2022-10-04 12:46:02,640 - INFO - Mean valid loss: 0.02556 score: 0.02547\n2022-10-04 12:46:02,642 - INFO - Starting 18 epoch...\n2022-10-04 12:46:04,071 - INFO - Mean train loss: 0.0248\n2022-10-04 12:46:06,363 - INFO - valid loss: 0.02541 valid loss aug: 0.02542\n2022-10-04 12:46:06,365 - INFO - Mean valid loss: 0.02542 score: 0.02534\n2022-10-04 12:46:06,368 - INFO - Starting 19 epoch...\n2022-10-04 12:46:07,793 - INFO - Mean train loss: 0.02396\n2022-10-04 12:46:10,101 - INFO - valid loss: 0.02549 valid loss aug: 0.02584\n2022-10-04 12:46:10,104 - INFO - Mean valid loss: 0.02567 score: 0.02544\n2022-10-04 12:46:10,107 - INFO - Starting 20 epoch...\n2022-10-04 12:46:11,543 - INFO - Mean train loss: 0.02394\n2022-10-04 12:46:13,854 - INFO - valid loss: 0.02568 valid loss aug: 0.02574\n2022-10-04 12:46:13,859 - INFO - Mean valid loss: 0.02571 score: 0.02559\n2022-10-04 12:46:13,861 - INFO - Starting 21 epoch...\n2022-10-04 12:46:15,281 - INFO - Mean train loss: 0.02337\n2022-10-04 12:46:17,576 - INFO - valid loss: 0.02567 valid loss aug: 0.0257\n2022-10-04 12:46:17,579 - INFO - Mean valid loss: 0.02569 score: 0.02547\n2022-10-04 12:46:17,581 - INFO - Starting 22 epoch...\n2022-10-04 12:46:19,035 - INFO - Mean train loss: 0.02361\n2022-10-04 12:46:21,340 - INFO - valid loss: 0.02568 valid loss aug: 0.02574\n2022-10-04 12:46:21,343 - INFO - Mean valid loss: 0.02571 score: 0.02546\n2022-10-04 12:46:21,345 - INFO - Starting 23 epoch...\n2022-10-04 12:46:22,770 - INFO - Mean train loss: 0.02424\n2022-10-04 12:46:25,114 - INFO - valid loss: 0.02535 valid loss aug: 0.02569\n2022-10-04 12:46:25,118 - INFO - Mean valid loss: 0.02552 score: 0.02532\n2022-10-04 12:46:25,126 - INFO - Starting 24 epoch...\n2022-10-04 12:46:26,589 - INFO - Mean train loss: 0.02345\n2022-10-04 12:46:28,924 - INFO - valid loss: 0.02559 valid loss aug: 0.02542\n2022-10-04 12:46:28,927 - INFO - Mean valid loss: 0.0255 score: 0.02536\n2022-10-04 12:46:28,929 - INFO - Starting 25 epoch...\n2022-10-04 12:46:30,389 - INFO - Mean train loss: 0.02339\n2022-10-04 12:46:32,803 - INFO - valid loss: 0.02559 valid loss aug: 0.02572\n2022-10-04 12:46:32,806 - INFO - Mean valid loss: 0.02566 score: 0.02548\n2022-10-04 12:46:32,808 - INFO - Starting 26 epoch...\n2022-10-04 12:46:34,254 - INFO - Mean train loss: 0.02379\n2022-10-04 12:46:36,560 - INFO - valid loss: 0.02568 valid loss aug: 0.02587\n2022-10-04 12:46:36,562 - INFO - Mean valid loss: 0.02578 score: 0.0256\n2022-10-04 12:46:36,565 - INFO - Starting 27 epoch...\n2022-10-04 12:46:37,995 - INFO - Mean train loss: 0.02326\n2022-10-04 12:46:40,317 - INFO - valid loss: 0.02566 valid loss aug: 0.02571\n2022-10-04 12:46:40,320 - INFO - Mean valid loss: 0.02568 score: 0.02543\n2022-10-04 12:46:40,323 - INFO - Starting 28 epoch...\n2022-10-04 12:46:41,780 - INFO - Mean train loss: 0.0233\n2022-10-04 12:46:44,075 - INFO - valid loss: 0.02561 valid loss aug: 0.02591\n2022-10-04 12:46:44,077 - INFO - Mean valid loss: 0.02576 score: 0.02541\n2022-10-04 12:46:44,080 - INFO - Starting 29 epoch...\n2022-10-04 12:46:45,600 - INFO - Mean train loss: 0.02329\n2022-10-04 12:46:47,908 - INFO - valid loss: 0.02551 valid loss aug: 0.02607\n2022-10-04 12:46:47,911 - INFO - Mean valid loss: 0.02579 score: 0.02545\n2022-10-04 12:46:47,913 - INFO - Starting 30 epoch...\n2022-10-04 12:46:49,346 - INFO - Mean train loss: 0.02342\n2022-10-04 12:46:51,677 - INFO - valid loss: 0.02578 valid loss aug: 0.02594\n2022-10-04 12:46:51,680 - INFO - Mean valid loss: 0.02586 score: 0.02551\n2022-10-04 12:46:51,682 - INFO - Starting 31 epoch...\n2022-10-04 12:46:53,154 - INFO - Mean train loss: 0.02377\n2022-10-04 12:46:55,498 - INFO - valid loss: 0.02631 valid loss aug: 0.02583\n2022-10-04 12:46:55,501 - INFO - Mean valid loss: 0.02607 score: 0.02581\n2022-10-04 12:46:55,503 - INFO - Starting 32 epoch...\n2022-10-04 12:46:56,962 - INFO - Mean train loss: 0.0238\n2022-10-04 12:46:59,276 - INFO - valid loss: 0.02569 valid loss aug: 0.02584\n2022-10-04 12:46:59,279 - INFO - Mean valid loss: 0.02576 score: 0.02537\n2022-10-04 12:46:59,282 - INFO - Starting 33 epoch...\n2022-10-04 12:47:00,723 - INFO - Mean train loss: 0.02411\n2022-10-04 12:47:03,072 - INFO - valid loss: 0.02602 valid loss aug: 0.02598\n2022-10-04 12:47:03,075 - INFO - Mean valid loss: 0.026 score: 0.02569\n2022-10-04 12:47:03,077 - INFO - Starting 34 epoch...\n2022-10-04 12:47:04,871 - INFO - Mean train loss: 0.02341\n2022-10-04 12:47:07,199 - INFO - valid loss: 0.02587 valid loss aug: 0.02579\n2022-10-04 12:47:07,202 - INFO - Mean valid loss: 0.02583 score: 0.02531\n2022-10-04 12:47:07,209 - INFO - Starting 35 epoch...\n2022-10-04 12:47:08,652 - INFO - Mean train loss: 0.02293\n2022-10-04 12:47:10,970 - INFO - valid loss: 0.02575 valid loss aug: 0.0263\n2022-10-04 12:47:10,972 - INFO - Mean valid loss: 0.02602 score: 0.02561\n2022-10-04 12:47:10,975 - INFO - Starting 36 epoch...\n2022-10-04 12:47:12,404 - INFO - Mean train loss: 0.02318\n2022-10-04 12:47:14,771 - INFO - valid loss: 0.02561 valid loss aug: 0.02563\n2022-10-04 12:47:14,774 - INFO - Mean valid loss: 0.02562 score: 0.02538\n2022-10-04 12:47:14,776 - INFO - Starting 37 epoch...\n2022-10-04 12:47:16,204 - INFO - Mean train loss: 0.02409\n2022-10-04 12:47:18,499 - INFO - valid loss: 0.02541 valid loss aug: 0.02564\n2022-10-04 12:47:18,502 - INFO - Mean valid loss: 0.02553 score: 0.02518\n2022-10-04 12:47:18,511 - INFO - Starting 38 epoch...\n2022-10-04 12:47:19,965 - INFO - Mean train loss: 0.02321\n2022-10-04 12:47:22,288 - INFO - valid loss: 0.02537 valid loss aug: 0.0257\n2022-10-04 12:47:22,290 - INFO - Mean valid loss: 0.02553 score: 0.0252\n2022-10-04 12:47:22,294 - INFO - Starting 39 epoch...\n2022-10-04 12:47:23,742 - INFO - Mean train loss: 0.0227\n2022-10-04 12:47:26,113 - INFO - valid loss: 0.02561 valid loss aug: 0.02581\n2022-10-04 12:47:26,116 - INFO - Mean valid loss: 0.02571 score: 0.02542\n2022-10-04 12:47:26,119 - INFO - Starting 40 epoch...\n2022-10-04 12:47:27,556 - INFO - Mean train loss: 0.0226\n2022-10-04 12:47:29,835 - INFO - valid loss: 0.02581 valid loss aug: 0.0263\n2022-10-04 12:47:29,838 - INFO - Mean valid loss: 0.02605 score: 0.02564\n2022-10-04 12:47:29,840 - INFO - Starting 41 epoch...\n2022-10-04 12:47:31,290 - INFO - Mean train loss: 0.02256\n2022-10-04 12:47:33,619 - INFO - valid loss: 0.02569 valid loss aug: 0.02588\n2022-10-04 12:47:33,621 - INFO - Mean valid loss: 0.02579 score: 0.02536\n2022-10-04 12:47:33,624 - INFO - Starting 42 epoch...\n2022-10-04 12:47:35,262 - INFO - Mean train loss: 0.02237\n2022-10-04 12:47:37,670 - INFO - valid loss: 0.0259 valid loss aug: 0.02631\n2022-10-04 12:47:37,674 - INFO - Mean valid loss: 0.02611 score: 0.02554\n2022-10-04 12:47:37,676 - INFO - Starting 43 epoch...\n2022-10-04 12:47:39,161 - INFO - Mean train loss: 0.02292\n2022-10-04 12:47:41,519 - INFO - valid loss: 0.02586 valid loss aug: 0.02604\n2022-10-04 12:47:41,523 - INFO - Mean valid loss: 0.02595 score: 0.02546\n2022-10-04 12:47:41,525 - INFO - Starting 44 epoch...\n2022-10-04 12:47:43,021 - INFO - Mean train loss: 0.02262\n2022-10-04 12:47:45,390 - INFO - valid loss: 0.02564 valid loss aug: 0.02612\n2022-10-04 12:47:45,393 - INFO - Mean valid loss: 0.02588 score: 0.02545\n2022-10-04 12:47:45,396 - INFO - Starting 45 epoch...\n2022-10-04 12:47:46,874 - INFO - Mean train loss: 0.02247\n2022-10-04 12:47:49,206 - INFO - valid loss: 0.0257 valid loss aug: 0.02622\n2022-10-04 12:47:49,209 - INFO - Mean valid loss: 0.02596 score: 0.02549\n2022-10-04 12:47:49,211 - INFO - Starting 46 epoch...\n2022-10-04 12:47:50,655 - INFO - Mean train loss: 0.02267\n2022-10-04 12:47:52,960 - INFO - valid loss: 0.02595 valid loss aug: 0.02614\n2022-10-04 12:47:52,963 - INFO - Mean valid loss: 0.02604 score: 0.0254\n2022-10-04 12:47:52,966 - INFO - Starting 47 epoch...\n2022-10-04 12:47:54,397 - INFO - Mean train loss: 0.02269\n2022-10-04 12:47:56,732 - INFO - valid loss: 0.02574 valid loss aug: 0.02605\n2022-10-04 12:47:56,735 - INFO - Mean valid loss: 0.0259 score: 0.0254\n2022-10-04 12:47:56,737 - INFO - Starting 48 epoch...\n2022-10-04 12:47:58,191 - INFO - Mean train loss: 0.0222\n2022-10-04 12:48:00,509 - INFO - valid loss: 0.02575 valid loss aug: 0.02603\n2022-10-04 12:48:00,511 - INFO - Mean valid loss: 0.02589 score: 0.02528\n2022-10-04 12:48:00,514 - INFO - Starting 49 epoch...\n2022-10-04 12:48:01,957 - INFO - Mean train loss: 0.02228\n2022-10-04 12:48:04,282 - INFO - valid loss: 0.02602 valid loss aug: 0.02628\n2022-10-04 12:48:04,285 - INFO - Mean valid loss: 0.02615 score: 0.02544\n2022-10-04 12:48:04,288 - INFO - Starting 50 epoch...\n2022-10-04 12:48:05,729 - INFO - Mean train loss: 0.02228\n2022-10-04 12:48:08,156 - INFO - valid loss: 0.02613 valid loss aug: 0.02597\n2022-10-04 12:48:08,158 - INFO - Mean valid loss: 0.02605 score: 0.02547\n2022-10-04 12:48:08,159 - INFO - best score=0.025179 on epoch=37 fold=4\n2022-10-04 12:48:08,160 - INFO - [train fold4] done in 190.39 s\n2022-10-04 12:48:08,162 - INFO - score avg=0.0243394, score fold0=0.024824, score fold1=0.02533, score fold2=0.024114, score fold3=0.02225, score fold4=0.025179\n2022-10-04 12:48:08,162 - INFO - [train] done in 956.32 s\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d3sGg6oC1io9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(torch.tensor(x_val, dtype=torch.float32)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:08.170695Z",
          "iopub.execute_input": "2022-10-04T12:48:08.171253Z",
          "iopub.status.idle": "2022-10-04T12:48:08.460109Z",
          "shell.execute_reply.started": "2022-10-04T12:48:08.171210Z",
          "shell.execute_reply": "2022-10-04T12:48:08.458779Z"
        },
        "trusted": true,
        "id": "o41lTYH01io-",
        "outputId": "d42e8166-6d25-45ce-f712-d176942277eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[-2.9345, -3.5135, -3.0092,  ..., -3.1132, -3.0310, -3.3582],\n        [-3.0662, -3.2412, -3.3177,  ..., -2.5900, -3.4198, -3.3080],\n        [-2.8059, -3.4830, -3.1034,  ..., -3.2263, -3.5511, -3.0009],\n        ...,\n        [-3.1106, -3.1485, -3.1191,  ..., -2.5150, -1.9142, -2.5577],\n        [-3.1417, -3.2420, -3.2113,  ..., -2.4473, -2.8685, -2.6741],\n        [-3.1915, -3.3236, -3.5163,  ..., -2.2839, -2.6101, -2.6754]],\n       grad_fn=<AddmmBackward0>)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = [torch.softmax(model(torch.tensor(x_val, dtype=torch.float32)), dim=1).float().detach().numpy() ]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:08.461830Z",
          "iopub.execute_input": "2022-10-04T12:48:08.462862Z",
          "iopub.status.idle": "2022-10-04T12:48:08.622350Z",
          "shell.execute_reply.started": "2022-10-04T12:48:08.462816Z",
          "shell.execute_reply": "2022-10-04T12:48:08.621090Z"
        },
        "trusted": true,
        "id": "XmQu-u6B1io-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_yards = []\n",
        "for i in range(len(y_preds[0])):\n",
        "    predict_yards.append(np.sum(y_preds[0][i]*np.arange(-99,100)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:08.624344Z",
          "iopub.execute_input": "2022-10-04T12:48:08.624787Z",
          "iopub.status.idle": "2022-10-04T12:48:08.638256Z",
          "shell.execute_reply.started": "2022-10-04T12:48:08.624746Z",
          "shell.execute_reply": "2022-10-04T12:48:08.636587Z"
        },
        "trusted": true,
        "id": "2bXhYeZQ1io-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.regplot(predict_yards,y_val_mae)\n",
        "plt.xlabel('Predicted Return Yards')\n",
        "plt.ylabel('Real Return Yards')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:08.642440Z",
          "iopub.execute_input": "2022-10-04T12:48:08.642890Z",
          "iopub.status.idle": "2022-10-04T12:48:09.159144Z",
          "shell.execute_reply.started": "2022-10-04T12:48:08.642849Z",
          "shell.execute_reply": "2022-10-04T12:48:09.157945Z"
        },
        "trusted": true,
        "id": "D_7Fme2_1io-",
        "outputId": "9a1559da-47f6-4429-ceb4-642a3dec9dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n  FutureWarning\n",
          "output_type": "stream"
        },
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Text(0, 0.5, 'Real Return Yards')"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x_punt = train[(train.PlayID == 1335) & (train.IsReturnTeam == True)]['x']\n",
        "test_x_return = train[(train.PlayID == 1335) & (train.IsReturnTeam == False)]['x']\n",
        "test_y_punt = train[(train.PlayID == 1335) & (train.IsReturnTeam == True)]['y']\n",
        "test_y_return = train[(train.PlayID == 1335) & (train.IsReturnTeam == False)]['y']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:09.160847Z",
          "iopub.execute_input": "2022-10-04T12:48:09.161191Z",
          "iopub.status.idle": "2022-10-04T12:48:09.175585Z",
          "shell.execute_reply.started": "2022-10-04T12:48:09.161160Z",
          "shell.execute_reply": "2022-10-04T12:48:09.174221Z"
        },
        "trusted": true,
        "id": "d4uzpiN11io-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_x_punt,test_y_punt)\n",
        "plt.scatter(test_x_return,test_y_return)\n",
        "plt.legend(('Punt_team','Return team'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:09.177923Z",
          "iopub.execute_input": "2022-10-04T12:48:09.178754Z",
          "iopub.status.idle": "2022-10-04T12:48:09.451504Z",
          "shell.execute_reply.started": "2022-10-04T12:48:09.178686Z",
          "shell.execute_reply": "2022-10-04T12:48:09.450248Z"
        },
        "trusted": true,
        "id": "xJJKrNVo1io-",
        "outputId": "c4145273-7664-4901-f00c-ec645ea1bf40"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 19,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<matplotlib.legend.Legend at 0x7f067a1e14d0>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:48:09.453624Z",
          "iopub.execute_input": "2022-10-04T12:48:09.454507Z",
          "iopub.status.idle": "2022-10-04T12:48:10.872209Z",
          "shell.execute_reply.started": "2022-10-04T12:48:09.454453Z",
          "shell.execute_reply": "2022-10-04T12:48:10.870790Z"
        },
        "trusted": true,
        "id": "LqfatFFu1io-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best score: {:.2f}%\".format(best_score * 3500))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-10-04T12:50:23.628755Z",
          "iopub.execute_input": "2022-10-04T12:50:23.629592Z",
          "iopub.status.idle": "2022-10-04T12:50:23.635781Z",
          "shell.execute_reply.started": "2022-10-04T12:50:23.629549Z",
          "shell.execute_reply": "2022-10-04T12:50:23.634789Z"
        },
        "trusted": true,
        "id": "j5wt_9zy1io-",
        "outputId": "9c97576b-a555-4048-f435-a5c088931447"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "best score: 88.13%\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uvu8wqVL1io-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}